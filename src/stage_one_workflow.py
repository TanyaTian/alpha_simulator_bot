import pandas as pd
from pydantic import BaseModel, Field, ValidationError
from typing import TypedDict, List, Optional, Dict, Any, Union
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
import json
import time
import re
from datetime import datetime
import os
from requests.exceptions import ConnectionError
from http.client import RemoteDisconnected
import concurrent.futures
import logging

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from ace_lib import (
    start_session, check_session_and_relogin, get_datafields,
    simulate_multi_alpha, get_check_submission, get_alpha_yearly_stats,
    get_simulation_result_json, get_operators, check_prod_corr_test,
    set_alpha_properties, get_alpha_pnl
)
from utils import extract_datafields, load_config, filter_alpha_by_datafields, validate_expression_fields, complete_expression, safe_api_call
from expression_validator import check_expression_syntax
from dao import StageOneSignalDAO
from logger import Logger
from self_corr_calculator import calc_self_corr, load_data, download_data, get_alpha_pnls
from cached_data_fetcher import get_datafields_with_cache

logger = Logger()
# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º DEBUG ä»¥æ–¹ä¾¿è°ƒè¯•
#logger.logger.setLevel(logging.DEBUG)

# --- 0. å®šä¹‰å¸¸é‡ ---
# å®šä¹‰å…è®¸çš„ç®—å­åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤ operators
ALLOWED_OPERATORS = [
    # --- Arithmetic ---
    #"add",
    #"subtract",
    #"multiply",
    #"divide",
    "abs",
    "sign",
    "log",
    "power",
    "signed_power",
    "sqrt",
    #"inverse",
    #"reverse",
    "max",
    "min",
    "round",
    "round_down",
    "arc_tan",

    # --- Logical / Comparison ---
    #"less",
    #"less_equal",
    #"greater",
    #"greater_equal",
    #"equal",
    #"not_equal",
    "and",
    "or",
    "not",
    "if_else",
    "is_nan",
    "is_not_finite",

    # --- Data Cleaning ---
    "pasteurize",
    "purify",
    "to_nan",
    "nan_mask",

    # --- Time Series (Stats & Structure) ---
    "ts_mean",
    "ts_std_dev",
    "ts_sum",
    "ts_product",
    "ts_max",
    "ts_min",
    "ts_delta",
    "ts_returns",
    "ts_decay_linear",
    "ts_av_diff",
    "ts_max_diff",
    "ts_kurtosis",
    "ts_delay",
    "days_from_last_change",
    "last_diff_value",
    "ts_step",
    "ts_count_nans",
    "ts_arg_max",
    "ts_arg_min",
    "ts_backfill",
    "jump_decay",
    "hump",
    "kth_element",
    "ts_zscore",
    "ts_skewness",

    # --- Vector (Intraday/Array) ---
    "vec_avg",
    "vec_sum",
    "vec_min",
    "vec_max",
    "vec_stddev",
    "vec_range",
    "vec_norm",
    "vec_count",

    # --- Transformation ---
    "clamp",
    "tail",
    "filter",
    "keep",
    "zscore",

    # --- å¤šå­—æ®µå…³ç³» ---
    "ts_corr",
    "ts_regression"
]


# --- Global Constants ---
NEUTRALIZATION_OPTIONS = ["REVERSION_AND_MOMENTUM",  "CROWDING", "FAST", "SLOW",  "SLOW_AND_FAST", "MARKET", "SECTOR", "INDUSTRY", "SUBINDUSTRY"]


# --- 1. å®šä¹‰ Pydantic æ¨¡å‹ç”¨äº LLM ç»“æ„åŒ–è¾“å‡º ---

class SearchDirection(BaseModel):
    """Stage1 èŠ‚ç‚¹è¾“å‡ºï¼šçº¯æ•°å­¦ç»Ÿè®¡é©±åŠ¨çš„ Alpha æœç´¢æ–¹å‘"""
    core_idea: str = Field(description="æ ¸å¿ƒæ•°å­¦/ç»Ÿè®¡ç›´è§‰ï¼Œä¸å«ä»»ä½•ç»æµå­¦è§£é‡Š")
    differentiation: str = Field(description="ä¸å·²å°è¯•alphaçš„å·®å¼‚ç‚¹ï¼šçª—å£/å˜æ¢è·¯å¾„/ç»“æ„ç±»å‹")
    target_structure: str = Field(description="é¢„æœŸæ•æ‰çš„ç»“æ„ç±»å‹ï¼šå¦‚æ³¢åŠ¨å¼‚å¸¸/å¤šå°ºåº¦èƒŒç¦»/åˆ†å¸ƒåæ–œ")
    constraints: str = Field(description="éœ€è¦é¿å…çš„é™·é˜±ï¼šè¿‡æ‹Ÿåˆå½¢æ€/é‡å¤è·¯å¾„/åˆ†å¸ƒåç¼©")

class DirectionNodeOutput(BaseModel):
    directions: List[SearchDirection] = Field(description="ä¸‰ä¸ªæœç´¢æ–¹å‘")

class ProposedAlpha(BaseModel):
    """è¡¨ç¤º LLM æå‡ºçš„å•ä¸ª Alpha å€™é€‰"""
    reasoning: str = Field(description="æå‡ºæ›´æ”¹çš„åŸå› ")
    new_expression: str = Field(description="æ–°çš„ã€ä¿®æ”¹åçš„ Alpha è¡¨è¾¾å¼")
    new_setting: dict = Field(description="åŒ…å« 'neutralization', 'decay', 'truncation' æ–°è®¾ç½®çš„å­—å…¸")

class AlphaProposal(BaseModel):
    """è¡¨ç¤ºä¸€æ‰¹æå‡ºçš„ Alpha å€™é€‰"""
    proposals: List[ProposedAlpha] = Field(description="ç”Ÿæˆçš„ alpha å€™é€‰åˆ—è¡¨")

# --- 2. å®šä¹‰å›¾çš„çŠ¶æ€ ---
class StageOneState(TypedDict):
    """è¡¨ç¤º stage one å·¥ä½œæµçš„çŠ¶æ€"""
    # --- è¾“å…¥å­—æ®µ ---
    session: Any  # è®¤è¯çš„ ace_lib ä¼šè¯
    dataset_ids: List[str]  # æ•°æ®é›† ID åˆ—è¡¨
    category: str # æ•°æ®é›†ç±»åˆ«
    region: str  # åŒºåŸŸ
    universe: str  # è‚¡ç¥¨æ± 
    delay: int  # å»¶è¿Ÿ
    date_time: str  # æ•°æ®æ—¶é—´ YYYYMMDD
    
    # --- æ•°æ®å­—æ®µ ---
    valid_data_fields: Any  # æœ‰æ•ˆæ•°æ®å­—æ®µ (DataFrame æˆ– List[Dict])
    search_directions: List[SearchDirection]  # æœç´¢æ–¹å‘åˆ—è¡¨
    
    # --- Alpha å€™é€‰ ---
    generated_expressions: List[ProposedAlpha]  # èŠ‚ç‚¹ 3 ç”Ÿæˆçš„ Alpha å€™é€‰ï¼ˆåŒ…å«è¡¨è¾¾å¼ã€è®¾ç½®å’ŒåŸå› ï¼‰
    valid_expressions: List[ProposedAlpha]  # èŠ‚ç‚¹ 3.5 éªŒè¯é€šè¿‡çš„ Alpha å€™é€‰ï¼Œå‡†å¤‡æäº¤ä»¿çœŸ
    
    # --- ä¸­é—´çŠ¶æ€å­—æ®µ (æ‹†åˆ† simulate_filter_and_save åæ–°å¢) ---
    current_batch_alpha_ids: List[str] # å½“å‰æ‰¹æ¬¡ä»¿çœŸç”Ÿæˆçš„ alpha id åˆ—è¡¨
    passed_filter_alphas: List[Dict[str, Any]] # é€šè¿‡è¿‡æ»¤ç­‰å¾…ç›¸å…³æ€§æ£€æŸ¥çš„ alpha ä¿¡æ¯ [{'id': 'xxx', 'fitness': 1.2}, ...]

    # --- å†å²è®°å½• ---
    historical_alphas: List[str]  # å†å²ç”Ÿæˆçš„ alpha è¡¨è¾¾å¼åˆ—è¡¨
    repeat_historical_alphas: List[str]  # é‡å¤ç”Ÿæˆçš„ alpha è¡¨è¾¾å¼åˆ—è¡¨
    saved_alphas_ids: List[str]  # å·²ä¿å­˜çš„ alpha ID åˆ—è¡¨
    
    # --- å·¥ä½œæµçŠ¶æ€å’Œç»“æœ ---
    iteration_count: int  # è¿­ä»£è®¡æ•°å™¨
    total_saved_count: int  # æ€»å…±ä¿å­˜çš„ alpha æ•°é‡
    insufficient_data_count: int  # æ•°æ®å†å²ä¸è¶³çš„ alpha æ•°é‡
    consecutive_empty_candidates: int # è¿ç»­äº§ç”Ÿçš„æ— æ•ˆå€™é€‰æ‰¹æ¬¡æ•°
    status: str  # çŠ¶æ€ï¼š'running', 'completed', 'failed'
    error_log: List[str]  # é”™è¯¯æ—¥å¿—

# --- LLM Setup ---
model = None

def load_llm_config():
    """Loads LLM configuration from config.ini using utils.load_config."""
    try:
        config_data = load_config("config/config.ini")
        if not config_data:
            logger.error(f"config_data is enpty")
            return {{}}
        # Use general LLM configuration (matches user's DeepSeek config)
        return {
            "model": config_data.get("llm_model"),
            "base_url": config_data.get("llm_base_url"),
            "api_key": config_data.get("llm_api_key")
        }
    except Exception as e:
        logger.error(f"Error loading LLM config: {e}")
        return {}

def initialize_model():
    """Initializes the single LLM used for batch generation."""
    global model
    llm_config = load_llm_config()
    if not all(llm_config.values()):
        logger.error("âŒ LLM configuration is missing or incomplete. Cannot initialize model.")
        return False
    try:
        model = ChatOpenAI(
            model=llm_config["model"],
            base_url=llm_config["base_url"],
            api_key=llm_config["api_key"],
            temperature=0.7, 
            request_timeout=180.0,
        )
        logger.info("âœ… LLM model initialized successfully.")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to initialize LLM model: {e}")
        return False

def get_model(temperature=0.7):
    """Helper to get a model instance with specific temperature if needed, or reuse global."""
    # Since ChatOpenAI is immutable regarding temperature after init usually,
    # and we want different temperatures, we might need to re-instantiate or use `bind` if supported.
    # For simplicity, we can just re-instantiate lightly or use the global one if temp doesn't matter too much.
    # But strictly following the logic:
    llm_config = load_llm_config()
    return ChatOpenAI(
        model=llm_config["model"],
        base_url=llm_config["base_url"],
        api_key=llm_config["api_key"],
        temperature=temperature,
        request_timeout=180.0,
    )

# --- 3. å®šä¹‰å›¾çš„èŠ‚ç‚¹ï¼ˆå‡½æ•°ï¼‰ ---

def initialize_and_load_data(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 1: åˆå§‹åŒ–å·¥ä½œæµçŠ¶æ€å¹¶åŠ è½½æ•°æ®
    å¦‚æœ valid_data_fields ä¸ºç©ºï¼Œåˆ™æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 1: åˆå§‹åŒ–ä¸æ•°æ®åŠ è½½ ---")
    logger.debug(f"Input State Keys: {state.keys()}")
    
    try:
        # 4. åŠ è½½æ•°æ®å­—æ®µ
        if state.get("valid_data_fields") is None or (isinstance(state["valid_data_fields"], (list, set)) and len(state["valid_data_fields"]) == 0) or (isinstance(state["valid_data_fields"], pd.DataFrame) and state["valid_data_fields"].empty):
            logger.info("å¼€å§‹åŠ è½½æ•°æ®å­—æ®µ...")
            session = check_session_and_relogin(state["session"])
            state["session"] = session
            
            all_fields_dfs = []
            
            # 4.1 åŠ è½½ç›®æ ‡æ•°æ®é›†å­—æ®µ
            for ds_id in state["dataset_ids"]:
                logger.info(f"è·å–æ•°æ®é›† {ds_id} çš„å­—æ®µ...")
                logger.debug(f"Calling get_datafields: region={state['region']}, universe={state['universe']}, delay={state['delay']}, dataset_id={ds_id}")
                df = get_datafields_with_cache(
                    session,
                    region=state["region"],
                    delay=state["delay"],
                    universe=state["universe"],
                    dataset_id=ds_id,
                    data_type="ALL"
                )
                if df is not None and not df.empty:
                    logger.debug(f"Retrieved {len(df)} fields for dataset {ds_id}")
                    all_fields_dfs.append(df)
                else:
                    logger.warning(f"No fields found for dataset {ds_id}")
            
            if not all_fields_dfs:
                error_msg = "æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®å­—æ®µ"
                logger.error(error_msg)
                state["status"] = "failed"
                state["error_log"].append(error_msg)
                logger.info(f"{task_info} --- èŠ‚ç‚¹ 1 ç»“æŸ: å¤±è´¥ (æ— æœ‰æ•ˆå­—æ®µ) ---")
                return state
            
            # åˆå¹¶æ‰€æœ‰å­—æ®µ
            combined_df = pd.concat(all_fields_dfs, ignore_index=True).drop_duplicates(subset=['id'])
            
            # è¿‡æ»¤æ‰ GROUP ç±»å‹çš„å­—æ®µï¼Œå› ä¸º Stage One ä¸ä½¿ç”¨åˆ†ç»„ç®—å­
            combined_df = combined_df[combined_df['type'] != 'GROUP']
            
            state["valid_data_fields"] = combined_df
            logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(combined_df)} ä¸ªå­—æ®µ")
            logger.debug(f"Combined fields sample: {combined_df.head(2).to_dict(orient='records')}")
        else:
            logger.info("æ•°æ®å­—æ®µå·²å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½")

    except Exception as e:
        logger.error(f"èŠ‚ç‚¹ 1 æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["status"] = "failed"
        state["error_log"].append(f"èŠ‚ç‚¹ 1 æ‰§è¡Œå¤±è´¥: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 1 ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state
    
    field_count = len(state["valid_data_fields"]) if isinstance(state["valid_data_fields"], pd.DataFrame) else 0
    logger.info(f"{task_info} --- èŠ‚ç‚¹ 1 ç»“æŸ: æˆåŠŸ (åŠ è½½ {field_count} ä¸ªå­—æ®µ) ---")
    return state

def generate_search_directions(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 2: ç”Ÿæˆæœç´¢æ–¹å‘
    è°ƒç”¨ LLM ç”Ÿæˆçº¯æ•°å­¦ç»Ÿè®¡é©±åŠ¨çš„ Alpha æœç´¢æ–¹å‘
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    
    # æ‰“å°æ›´æ˜¾è‘—çš„è¿­ä»£æ±‡æ€»æ—¥å¿—ï¼ˆæ”¾åœ¨æ¯æ¬¡è¿­ä»£çš„æœ€å¼€å§‹ï¼‰
    total_historical = len(state.get("historical_alphas", []))
    total_repeat = len(state.get("repeat_historical_alphas", []))
    total_generated = total_historical + total_repeat
    repeat_rate = (total_repeat / total_generated * 100) if total_generated > 0 else 0
    
    logger.info(f"\n{'='*80}\n"
                f"ğŸ”„ è¿­ä»£æ±‡æ€» | Iteration: {state.get('iteration_count', 0)}\n"
                f"ğŸ“ˆ å†å² Alpha æ€»æ•°: {total_historical}\n"
                f"ğŸ”„ é‡å¤ Alpha æ€»æ•°: {total_repeat}\n"
                f"ğŸ“‰ æ•°æ®ä¸è¶³ç§»é™¤: {state.get('insufficient_data_count', 0)}\n"
                f"ğŸ“Š Alpha è¡¨è¾¾å¼é‡å¤ç”Ÿæˆç‡: {repeat_rate:.2f}%\n"
                f"âœ… å½“å‰ç´¯è®¡ä¿å­˜: {state.get('total_saved_count', 0)}\n"
                f"{'='*80}")
    
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 2: ç”Ÿæˆæœç´¢æ–¹å‘ ---")
    
    try:
        # å‡†å¤‡æ•°æ®
        valid_fields_df = state["valid_data_fields"]
        if isinstance(valid_fields_df, pd.DataFrame):
            dataset_fields_str = valid_fields_df[['id', 'description', 'type', 'coverage']].to_string(index=False)
        else:
             dataset_fields_str = str(valid_fields_df)
        
        logger.debug(f"Using dataset fields string length: {len(dataset_fields_str)}")

        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        try:
            with open(os.path.join(project_root, "data/prompt/search_direction_prompt_v2.md"), "r") as f:
                prompt_template = f.read()
            with open(os.path.join(project_root, "data/prompt/math_stat_thinking_v2.md"), "r") as f:
                math_stat_thinking = f.read()
        except Exception as e:
             logger.error(f"è¯»å– Prompt æ–‡ä»¶å¤±è´¥: {e}")
             raise e

        # å¡«å…… Prompt
        prompt = prompt_template.replace("{dataset_fields}", dataset_fields_str)
        prompt = prompt.replace("{math_stat_thinking}", math_stat_thinking)
        
        # Add explicit JSON instruction
        prompt += "\n\nCRITICAL: Return the output as a valid JSON object with a single key 'directions' containing a list of objects. Each object must have 'core_idea', 'differentiation', 'target_structure', and 'constraints' fields."
        
        logger.debug(f"Prompt constructed. Length: {len(prompt)}")

        logger.info("è°ƒç”¨ LLM ç”Ÿæˆæœç´¢æ–¹å‘ (Temperature=0.8)...")
        
        # Instantiate model with specific temp
        llm = get_model(temperature=0.8)
        
        max_retries = 3
        attempts = 0
        while attempts < max_retries:
            try:
                logger.debug(f"Calling LLM invoke (attempt {attempts + 1})...")
                response = llm.invoke(prompt)
                
                # Parsing logic
                content = response.content
                logger.debug(f"Raw LLM response (first 500 chars): {content[:500]}...")

                json_start = content.find('```json')
                if json_start != -1:
                    # Try to find the closing backticks after the opening tag
                    json_end = content.find('```', json_start + 7)
                    
                    if json_end != -1:
                        json_str = content[json_start + 7:json_end]
                    else:
                        # Fallback: assume the rest of the string is JSON (maybe missing closing backticks)
                        logger.warning("Closing backticks not found, attempting to parse from start tag to end.")
                        json_str = content[json_start + 7:]
                else:
                    # Try to find { or [
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group()
                    else:
                         raise ValueError("No JSON found in response")
                
                json_str = json_str.strip()
                logger.debug(f"Extracted JSON string length: {len(json_str)}")
                
                if not json_str:
                     raise ValueError("Extracted JSON string is empty")

                parsed_json = json.loads(json_str)
                # If parsed_json is a list, wrap it
                if isinstance(parsed_json, list):
                    parsed_json = {"directions": parsed_json}
                
                validated = DirectionNodeOutput.model_validate(parsed_json)
                state["search_directions"] = validated.directions
                logger.info(f"ç”Ÿæˆäº† {len(state['search_directions'])} ä¸ªæœç´¢æ–¹å‘:\n{json.dumps([d.model_dump() for d in state['search_directions']], indent=2, ensure_ascii=False)}")
                break

            except (json.JSONDecodeError, ValueError, ValidationError, RemoteDisconnected, ConnectionError) as e:
                attempts += 1
                logger.warning(f"Attempt {attempts}/{max_retries}: Failed to parse or validate LLM response: {e}")
                if attempts >= max_retries:
                    state["error_log"].append(f"Final attempt failed to parse LLM response in Node 2: {e}")
                time.sleep(5)
            except Exception as e:
                attempts += 1
                logger.error(f"Attempt {attempts}/{max_retries}: Error during LLM call in Node 2: {e}")
                if attempts >= max_retries:
                    state["error_log"].append(f"Final attempt failed during LLM call in Node 2: {e}")
                time.sleep(5)

    except Exception as e:
        logger.error(f"èŠ‚ç‚¹ 2 æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["error_log"].append(f"èŠ‚ç‚¹ 2 æ‰§è¡Œå¤±è´¥: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 2 ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state
    
    logger.info(f"{task_info} --- èŠ‚ç‚¹ 2 ç»“æŸ: æˆåŠŸ (ç”Ÿæˆ {len(state.get('search_directions', []))} ä¸ªæ–¹å‘) ---")
    return state

def generate_alpha_candidates(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 3: ç”Ÿæˆ Alpha å€™é€‰
    åŸºäºæœç´¢æ–¹å‘ç”Ÿæˆå…·ä½“çš„ Alpha è¡¨è¾¾å¼
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 3: ç”Ÿæˆ Alpha å€™é€‰ (å¹¿åº¦æœç´¢æ¨¡å¼) ---")
    
    # 0. æ£€æŸ¥ search_directionsï¼Œå¦‚æœä¸ºç©ºåˆ™è·³è¿‡å¤§æ¨¡å‹è°ƒç”¨
    search_directions = state.get("search_directions", [])
    if not search_directions:
        logger.warning(f"{task_info} --- èŠ‚ç‚¹ 3: è·³è¿‡å¤§æ¨¡å‹è°ƒç”¨ (æ— æœç´¢æ–¹å‘) ---")
        state["generated_expressions"] = []
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 3 ç»“æŸ: è·³è¿‡ (æ— æœç´¢æ–¹å‘) ---")
        return state
        
    try:
        session = check_session_and_relogin(state["session"])
        state["session"] = session
        
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # --- å‡†å¤‡é™æ€èµ„æº (åªåšä¸€æ¬¡) ---
        
        # 1. å‡†å¤‡ OPERATORS
        operators_df = safe_api_call(get_operators, session)
        regular_ops = operators_df[operators_df['scope'] == 'REGULAR']
        filtered_ops = regular_ops[regular_ops['name'].isin(ALLOWED_OPERATORS)]
        operators_json = filtered_ops.to_json(orient='records')
        logger.debug(f"Available operators count after filtering: {len(filtered_ops)}")
        
        # 2. å‡†å¤‡ DATAFIELDS
        valid_fields_df = state["valid_data_fields"]
        datafields_str = valid_fields_df[['id', 'description', 'type', 'coverage']].to_string(index=False) if isinstance(valid_fields_df, pd.DataFrame) else str(valid_fields_df)
        
        # 3. å‡†å¤‡ ALPHA_EXAMPLES
        try:
            with open(os.path.join(project_root, "data/prompt/alpha_examples_v2.md"), "r") as f:
                alpha_examples_str = f.read()
            if not alpha_examples_str.strip():
                alpha_examples_str = "æ— ç¤ºä¾‹"
        except Exception:
            alpha_examples_str = "æ— ç¤ºä¾‹"

        # 4. è¯»å– Prompt æ¨¡æ¿
        try:
            with open(os.path.join(project_root, "data/prompt/generate_alpha_prompt_v2.md"), "r") as f:
                prompt_template_raw = f.read()
        except Exception as e:
            logger.error(f"è¯»å– Prompt æ–‡ä»¶å¤±è´¥: {e}")
            raise e

        # 5. åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        all_proposals = []
        
        # 6. å¾ªç¯å¤„ç†æ¯ä¸ªæœç´¢æ–¹å‘
        total_directions = len(search_directions)
        
        for idx, direction in enumerate(search_directions):
            logger.info(f"ğŸ‘‰ æ­£åœ¨å¤„ç†æ–¹å‘ {idx + 1}/{total_directions}: {direction.core_idea[:50]}...")
            
            # æ„é€ è¯¥æ–¹å‘çš„æè¿°å­—ç¬¦ä¸²
            direction_str = f"Search Direction:\n"
            direction_str += f"  Core Idea: {direction.core_idea}\n"
            direction_str += f"  Differentiation: {direction.differentiation}\n"
            direction_str += f"  Target Structure: {direction.target_structure}\n"
            direction_str += f"  Constraints: {direction.constraints}\n"
            
            # å¡«å…… Prompt
            prompt = prompt_template_raw.replace("{SEARCH_DIRECTIONS}", direction_str)
            prompt = prompt.replace("{OPERATORS}", operators_json)
            prompt = prompt.replace("{DATAFIELDS}", datafields_str)
            prompt = prompt.replace("{ALPHA_EXAMPLES}", alpha_examples_str)
            prompt = prompt.replace("{NEUTRALIZATION_OPTIONS}", " ".join(NEUTRALIZATION_OPTIONS))
            
            # æ·»åŠ å¹¿åº¦æœç´¢å’Œæ•°é‡æŒ‡ä»¤
            # åŠ¨æ€è°ƒæ•´è¯·æ±‚æ•°é‡ï¼Œå¦‚æœæœ‰å¤§é‡å­—æ®µï¼Œé™åˆ¶ä¸Šé™ä»¥é˜² token æº¢å‡º
            field_count = len(valid_fields_df) if isinstance(valid_fields_df, pd.DataFrame) else 0
            if field_count > 50:
                prompt = prompt.replace("{COUNT}", "100")
            else:
                prompt = prompt.replace("{COUNT}", "50")

            prompt += "\n\nCRITICAL: Return the output as a valid JSON object with a single key 'proposals' containing a list of objects. Each object must have 'reasoning', 'new_expression', and 'new_setting'."

            # è°ƒç”¨ LLM
            llm = get_model(temperature=0.2) # ç¨å¾®æé«˜æ¸©åº¦ä»¥å¢åŠ å¤šæ ·æ€§ï¼Œä½†ä¿æŒè¶³å¤Ÿçš„ç»“æ„ç¨³å®šæ€§
            
            max_retries = 3
            attempts = 0
            success = False
            
            while attempts < max_retries:
                try:
                    logger.debug(f"Calling LLM invoke for direction {idx + 1} (attempt {attempts + 1})...")
                    response = llm.invoke(prompt)
                    
                    # è§£æå“åº”
                    content = response.content
                    json_start = content.find('```json')
                    if json_start != -1:
                        json_end = content.find('```', json_start + 7)
                        if json_end != -1:
                            json_str = content[json_start + 7:json_end]
                        else:
                            json_str = content[json_start + 7:]
                    else:
                        json_match = re.search(r'\{.*\}', content, re.DOTALL) # å°è¯•åŒ¹é…æœ€å¤–å±‚å¤§æ‹¬å· (å¯èƒ½ä¸åŒ…å« proposals åˆ—è¡¨ï¼Œéœ€å°å¿ƒ)
                        # æ›´å¥å£®çš„ fallback: æ‰¾åˆ—è¡¨å¼€å§‹
                        list_match = re.search(r'\[.*\]', content, re.DOTALL)
                        if json_match:
                             json_str = json_match.group()
                        elif list_match:
                             # è¿™ç§æƒ…å†µä¸‹æ¨¡å‹å¯èƒ½ç›´æ¥è¿”å›äº†åˆ—è¡¨
                             json_str = f'{{"proposals": {list_match.group()}}}'
                        else:
                             # æœ€åçš„å°è¯•ï¼šå‡è®¾æ•´ä¸ª content å°±æ˜¯ json
                             json_str = content

                    json_str = json_str.strip()
                    if not json_str:
                        raise ValueError("Extracted JSON string is empty")

                    parsed_json = json.loads(json_str)
                    
                    # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨è€Œé dict
                    if isinstance(parsed_json, list):
                        parsed_json = {"proposals": parsed_json}
                        
                    validated_proposals = AlphaProposal.model_validate(parsed_json)
                    new_proposals = validated_proposals.proposals
                    
                    if new_proposals:
                        all_proposals.extend(new_proposals)
                        logger.info(f"æ–¹å‘ {idx + 1} ç”Ÿæˆäº† {len(new_proposals)} ä¸ª Alpha å€™é€‰ã€‚")
                        success = True
                        break
                    else:
                        logger.warning(f"æ–¹å‘ {idx + 1} è¿”å›äº†ç©ºçš„ proposals åˆ—è¡¨ã€‚")
                        break

                except (json.JSONDecodeError, ValueError, ValidationError, RemoteDisconnected, ConnectionError) as e:
                    attempts += 1
                    logger.warning(f"Direction {idx + 1} Attempt {attempts}/{max_retries}: Failed to parse LLM response: {e}")
                    time.sleep(3)
                except Exception as e:
                    attempts += 1
                    logger.error(f"Direction {idx + 1} Attempt {attempts}/{max_retries}: Error during LLM call: {e}")
                    time.sleep(3)
            
            if not success:
                logger.error(f"æ–¹å‘ {idx + 1} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡ã€‚")
                state["error_log"].append(f"æ–¹å‘ {idx + 1} ç”Ÿæˆå¤±è´¥")

        state["generated_expressions"] = all_proposals
        logger.info(f"æ‰€æœ‰æ–¹å‘å¤„ç†å®Œæ¯•ã€‚æ€»å…±ç”Ÿæˆ {len(state['generated_expressions'])} ä¸ª Alpha å€™é€‰ã€‚")

    except Exception as e:
        logger.error(f"èŠ‚ç‚¹ 3 æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["error_log"].append(f"èŠ‚ç‚¹ 3 æ‰§è¡Œå¤±è´¥: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 3 ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state
    
    logger.info(f"{task_info} --- èŠ‚ç‚¹ 3 ç»“æŸ: æˆåŠŸ (ç”Ÿæˆ {len(state.get('generated_expressions', []))} ä¸ªå€™é€‰) ---")
    return state

def validate_alpha_candidates(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 3.5: éªŒè¯ Alpha å€™é€‰
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 3.5: éªŒè¯ Alpha å€™é€‰ ---")
    
    try:
        session = state["session"]
        generated_expressions = state["generated_expressions"]
        logger.debug(f"Starting validation for {len(generated_expressions)} candidates.")
        
        # 1. å‡†å¤‡éªŒè¯æ‰€éœ€çš„å­—æ®µé›†åˆ
        valid_fields_df = state["valid_data_fields"]
        if isinstance(valid_fields_df, pd.DataFrame):
            valid_field_ids = set(valid_fields_df['id'].unique())
        else:
            valid_field_ids = set()
        logger.debug(f"Number of valid field IDs: {len(valid_field_ids)}")
            
        # 2. æ„é€  settings å­—å…¸
        settings_dict = {
            "region": state["region"],
            "universe": state["universe"],
            "delay": state["delay"]
        }
        
        # é¢„å…ˆè·å– operatorsï¼Œé¿å…åœ¨ check_expression_syntax ä¸­å¤šæ¬¡è°ƒç”¨å¯¼è‡´è¿æ¥é£é™©
        stage_one_operators_df = None
        try:
            operators_df = safe_api_call(get_operators, session)
            regular_operators_df = operators_df[operators_df['scope'] == 'REGULAR']
            stage_one_operators_df = regular_operators_df[regular_operators_df['name'].isin(ALLOWED_OPERATORS)]
            logger.debug("Successfully pre-fetched operators for validation.")
        except Exception as e:
            logger.warning(f"Failed to pre-fetch operators: {e}. Validation might fail or be slow.")

        valid_candidates = []
        for candidate in generated_expressions:
            expression = candidate.new_expression
            logger.debug(f"Validating candidate expression: {expression}")
            
            if not expression: 
                logger.warning(f"è·³è¿‡æ— è¡¨è¾¾å¼çš„å€™é€‰: {candidate.reasoning}")
                continue
            
            # 3. è¿‡æ»¤æ‰ä»…ä½¿ç”¨ pv1 å­—æ®µçš„ alpha
            if not filter_alpha_by_datafields(expression, settings_dict, session):
                logger.info(f"Alpha '{expression}' ä»…åŒ…å« pv1 å­—æ®µï¼Œè¢«è¿‡æ»¤ã€‚")
                continue
                
            # 4. éªŒè¯å­—æ®µæ˜¯å¦å…¨éƒ¨åœ¨æœ‰æ•ˆå­—æ®µé›†ä¸­
            is_fields_valid, field_error = validate_expression_fields(expression, valid_field_ids, session)
            if not is_fields_valid:
                logger.warning(f"å­—æ®µéªŒè¯å¤±è´¥ '{expression}': {field_error}")
                continue
                
            # 5. éªŒè¯è¯­æ³•
            # ä¼ å…¥é¢„å…ˆè·å–çš„ operators_df
            is_syntax_valid, syntax_errors = check_expression_syntax(expression, session, operators_df=stage_one_operators_df)
            if not is_syntax_valid:
                logger.warning(f"è¯­æ³•éªŒè¯å¤±è´¥ '{expression}': {syntax_errors}")
                # è®°å½•è¯¦ç»†é”™è¯¯
                with open("syntax_error_log.txt", "a") as f:
                    f.write(f"--- Syntax Error ---\n")
                    f.write(f"Expression: {expression}\n")
                    f.write(f"Reason: {syntax_errors}\n\n")
                continue
                
            # 6. è‡ªåŠ¨è¡¥å…¨å’Œä¿®æ­£è¡¨è¾¾å¼ (å¦‚æ·»åŠ  vec_avg ç­‰)
            try:
                # complete_expression è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ª
                # æ³¨æ„ï¼šcomplete_expression éœ€è¦ valid_data_fields ä¸º DataFrame ä»¥è·å–ç±»å‹ä¿¡æ¯
                completed_expressions = complete_expression(expression, valid_fields_df, session)
                if completed_expressions:
                    processed_expression = completed_expressions[0]
                    logger.info(f"éªŒè¯é€šè¿‡å¹¶å¤„ç†: {processed_expression}")
                    logger.debug(f"Original: {expression} -> Processed: {processed_expression}")
                    
                    # æ›´æ–°å€™é€‰å¯¹è±¡çš„è¡¨è¾¾å¼
                    # ç”±äº ProposedAlpha æ˜¯ Pydantic æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„å®ä¾‹æˆ–è€…ç›´æ¥ä¿®æ”¹
                    # è¿™é‡Œé€‰æ‹©æ›´æ–° new_expressionï¼Œå› ä¸ºåç»­æ­¥éª¤ç›´æ¥ä½¿ç”¨å®ƒ
                    candidate.new_expression = processed_expression
                    valid_candidates.append(candidate)
                else:
                    logger.info(f"è¡¨è¾¾å¼å¤„ç†è¿”å›ç©ºåˆ—è¡¨: {expression}")
            except Exception as e:
                logger.error(f"è¡¨è¾¾å¼å¤„ç†å¼‚å¸¸ '{expression}': {e}")
        
        state["valid_expressions"] = valid_candidates
        logger.info(f"éªŒè¯é€šè¿‡ {len(valid_candidates)}/{len(generated_expressions)}")
        
        # æ›´æ–°è¿ç»­ç©ºè½¬è®¡æ•°
        if not valid_candidates:
            state["consecutive_empty_candidates"] += 1
        else:
            state["consecutive_empty_candidates"] = 0
            
    except Exception as e:
        logger.error(f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        state["error_log"].append(f"éªŒè¯è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 3.5 ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state
    
    logger.info(f"{task_info} --- èŠ‚ç‚¹ 3.5 ç»“æŸ: æˆåŠŸ (é€šè¿‡ {len(valid_candidates)}/{len(generated_expressions)} ä¸ª) ---")
    return state

def simulate_alphas(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 4a: æ¨¡æ‹ŸèŠ‚ç‚¹
    èŒè´£ï¼šå°† valid_expressions æ‹¼æˆ sim_jobsï¼Œæ¯æ¬¡åªå– 10 ä¸ª alphaã€‚
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 4a: æ¨¡æ‹Ÿ Alphas ---")
    
    try:
        session = check_session_and_relogin(state["session"])
        state["session"] = session
        
        # è·å–å¾…ä»¿çœŸçš„è¡¨è¾¾å¼åˆ—è¡¨
        all_valid_expressions = state.get("valid_expressions", [])
        
        if not all_valid_expressions:
            logger.info("æ— å¾…ä»¿çœŸè¡¨è¾¾å¼ã€‚")
            state["current_batch_alpha_ids"] = []
            logger.info(f"{task_info} --- èŠ‚ç‚¹ 4a ç»“æŸ: è·³è¿‡ (æ— è¡¨è¾¾å¼) ---")
            return state
            
        # æ¯æ¬¡åªå–å‰ 10 ä¸ª
        batch_size = 10
        current_batch = all_valid_expressions[:batch_size]
        remaining_expressions = all_valid_expressions[batch_size:]
        
        # æ›´æ–° state ä¸­çš„ valid_expressions (ç§»é™¤å·²å–å‡ºçš„)
        state["valid_expressions"] = remaining_expressions
        
        logger.info(f"æœ¬è½®ä»¿çœŸ {len(current_batch)} ä¸ªè¡¨è¾¾å¼ï¼Œå‰©ä½™ {len(remaining_expressions)} ä¸ªå¾…ä»¿çœŸã€‚")

        # 1. å‡†å¤‡ä»¿çœŸä»»åŠ¡
        sim_jobs = []
        for candidate in current_batch:
            sim_settings = {
                "instrumentType": "EQUITY",
                "region": state["region"],
                "universe": state["universe"],
                "delay": state["delay"],
                "decay": candidate.new_setting.get("decay", 0),
                "neutralization": candidate.new_setting.get("neutralization", "INDUSTRY"),
                "truncation": candidate.new_setting.get("truncation", 0.08),
                "pasteurization": "ON",
                "testPeriod": "P2Y",
                "unitHandling": "VERIFY",
                "nanHandling": "OFF",
                "maxTrade": "OFF",
                "language": "FASTEXPR",
                "visualization": False
            }
            # ç¡®ä¿è¡¨è¾¾å¼æ˜¯å­—ç¬¦ä¸²
            expression = candidate.new_expression
            sim_jobs.append({
                "type": "REGULAR",
                "settings": sim_settings,
                "regular": expression
            })
            
        # 2. æ‰§è¡Œæ‰¹é‡ä»¿çœŸ
        batch_results = []
        max_retries = 3
        attempts = 0
        timeout_duration = 30 * 60 # 30åˆ†é’Ÿè¶…æ—¶

        while attempts < max_retries:
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            try:
                logger.info(f"è¿è¡Œæ‰¹é‡ä»¿çœŸ... (å°è¯• {attempts + 1}/{max_retries})")
                future = executor.submit(simulate_multi_alpha, session, sim_jobs)
                batch_results = future.result(timeout=timeout_duration)
                executor.shutdown(wait=True)
                logger.info("æ‰¹é‡ä»¿çœŸå®Œæˆã€‚")
                break
            except concurrent.futures.TimeoutError as e:
                # å…³é”®ä¿®æ”¹ï¼šè¶…æ—¶åä¸ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œç›´æ¥å…³é—­ executor å¹¶æŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•
                logger.warning(f"ä»¿çœŸè¶…æ—¶ (å°è¯• {attempts + 1})ï¼Œæ­£åœ¨å¼ºåˆ¶ç»§ç»­...")
                executor.shutdown(wait=False) 
                attempts += 1
                error_msg = f"ä»¿çœŸå°è¯• {attempts}/{max_retries} å¤±è´¥: {e}"
                logger.error(error_msg)
                state["error_log"].append(error_msg)
                if attempts >= max_retries:
                    logger.error("ä»¿çœŸæœ€ç»ˆå¤±è´¥ã€‚")
                    batch_results = []
                    break
                time.sleep(60)
                session = start_session()
                state["session"] = session
            except (RemoteDisconnected, ConnectionError, Exception) as e:
                executor.shutdown(wait=False)
                attempts += 1
                error_msg = f"ä»¿çœŸå°è¯• {attempts}/{max_retries} å¤±è´¥: {e}"
                logger.error(error_msg)
                state["error_log"].append(error_msg)
                if attempts >= max_retries:
                    logger.error("ä»¿çœŸæœ€ç»ˆå¤±è´¥ã€‚")
                    batch_results = []
                    break
                time.sleep(60)
                session = start_session()
                state["session"] = session

        # æå– Alpha IDs
        current_alpha_ids = []
        for res in batch_results:
            aid = res.get("alpha_id")
            if aid and aid not in current_alpha_ids:
                current_alpha_ids.append(aid)
        
        state["current_batch_alpha_ids"] = current_alpha_ids
        
    except Exception as e:
        logger.error(f"æ¨¡æ‹ŸèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["error_log"].append(f"æ¨¡æ‹ŸèŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
        state["current_batch_alpha_ids"] = []
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 4a ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state

    logger.info(f"{task_info} --- èŠ‚ç‚¹ 4a ç»“æŸ: æˆåŠŸ (è·å¾— {len(state.get('current_batch_alpha_ids', []))} ä¸ª ID) ---")
    return state

def filter_alphas(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 4b: è¿‡æ»¤èŠ‚ç‚¹
    èŒè´£ï¼šæ£€æŸ¥ year state å’Œ check ä¿¡æ¯ï¼Œç”Ÿæˆåè½¬ä¿¡å·ï¼Œç­›é€‰é€šè¿‡çš„ alphaã€‚
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 4b: è¿‡æ»¤ Alphas ---")
    
    try:
        session = check_session_and_relogin(state["session"])
        state["session"] = session
        
        current_alpha_ids = state.get("current_batch_alpha_ids", [])
        if not current_alpha_ids:
            logger.info("æœ¬æ‰¹æ¬¡æ—  Alpha IDï¼Œè·³è¿‡è¿‡æ»¤ã€‚")
            logger.info(f"{task_info} --- èŠ‚ç‚¹ 4b ç»“æŸ: è·³è¿‡ (æ—  ID) ---")
            return state

        # åˆå§‹åŒ– passed_filter_alphas (å¦‚æœä¸å­˜åœ¨)
        if "passed_filter_alphas" not in state or state["passed_filter_alphas"] is None:
            state["passed_filter_alphas"] = []

        def _get_json_from_url(sess, url):
            response = sess.get(url)
            response.raise_for_status()
            return response.json()
        
        passed_count_this_run = 0

        for alpha_id in current_alpha_ids:
            logger.debug(f"æ­£åœ¨æ£€æŸ¥ Alpha: {alpha_id}")
            
            # è·å–è¡¨è¾¾å¼ (ç”¨äºè®°å½•å†å²å’Œç”Ÿæˆåè½¬ä¿¡å·)
            expression = ""
            try:
                alpha_details = safe_api_call(_get_json_from_url, session, f"https://api.worldquantbrain.com/alphas/{alpha_id}")
                if alpha_details:
                    expression = alpha_details.get('regular', {}).get('code', "")
            except Exception as e:
                logger.error(f"è·å– Alpha {alpha_id} è¯¦æƒ…å¤±è´¥: {e}")

            # è·å–ç»Ÿè®¡å’Œæ£€æŸ¥æ•°æ®
            stats_df = safe_api_call(get_alpha_yearly_stats, session, alpha_id)
            check_df = safe_api_call(get_check_submission, session, alpha_id)

            # --- 1. Year State æ£€æŸ¥ (Data History) ---
            active_years = 0
            if stats_df is not None and not stats_df.empty:
                active_years = (stats_df['sharpe'] != 0.0).sum()
            
            if active_years < 8:
                logger.info(f"Alpha {alpha_id} æ•°æ®å†å²ä¸è¶³ ({active_years} å¹´)ï¼Œç›´æ¥æ”¾å¼ƒå¹¶ä»å†å²è®°å½•ç§»é™¤ã€‚")
                
                # å¢åŠ è®¡æ•°
                state["insufficient_data_count"] = state.get("insufficient_data_count", 0) + 1

                # å°è¯•ç§»é™¤å¯¼è‡´æ•°æ®ä¸è¶³çš„å­—æ®µ
                if 0 < active_years < 8 and expression:
                    try:
                        used_fields = extract_datafields(expression, session)
                        valid_fields_df = state["valid_data_fields"]
                        if isinstance(valid_fields_df, pd.DataFrame):
                             fields_to_remove = [f for f in used_fields if f in valid_fields_df['id'].values]
                             if fields_to_remove:
                                 logger.info(f"ç§»é™¤å¯¼è‡´æ•°æ®ä¸è¶³çš„å­—æ®µ: {fields_to_remove}")
                                 state["valid_data_fields"] = valid_fields_df[~valid_fields_df['id'].isin(fields_to_remove)]
                    except Exception as e:
                        logger.error(f"ç§»é™¤å­—æ®µå¤±è´¥: {e}")
                continue # Skip processing this alpha
            else:
                if expression:
                    if expression in state["historical_alphas"]:
                        # å¦‚æœå·²å­˜åœ¨ï¼Œæ·»åŠ åˆ° repeat_historical_alphas åˆ—è¡¨
                        state["repeat_historical_alphas"].append(expression)
                    else:
                        # å¦‚æœä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ° historical_alphas åˆ—è¡¨
                        state["historical_alphas"].append(expression)

            # --- 2. Check ä¿¡æ¯æ£€æŸ¥ ---
            pass_check = True
            fail_reason = ""
            fitness = 0.0
            sharpe = 0.0
            
            if check_df is not None and not check_df.empty:
                checks = check_df.set_index('name')['value'].to_dict()
                checks_result = check_df.set_index('name')['result'].to_dict()
                
                sharpe = checks.get('LOW_SHARPE', 0)
                fitness = checks.get('LOW_FITNESS', 0)
                robust = checks.get('LOW_ROBUST_UNIVERSE_SHARPE', 1.0)
                jpn_robust = checks.get('LOW_ASI_JPN_SHARPE', 1.0)
                conc_weight_res = checks_result.get('CONCENTRATED_WEIGHT', 'PASS')
                
                if sharpe <= 1.0:
                    pass_check = False
                    fail_reason = f"Sharpe {sharpe:.2f} <= 1.0"
                elif fitness <= 0.3: # æ³¨æ„ï¼šPrompt ä¸­è¯´ fitness < -0.3 è§¦å‘åè½¬ï¼Œè¿™é‡Œæ˜¯é€šè¿‡æ ‡å‡† fitness > 0.3? Prompt æ²¡ç»†è¯´é€šè¿‡æ ‡å‡†ï¼Œæ²¿ç”¨æ—§é€»è¾‘ > 0.3
                    pass_check = False
                    fail_reason = f"Fitness {fitness:.2f} <= 0.3"
                elif robust <= 0.8:
                    pass_check = False
                    fail_reason = f"Robust {robust:.2f} <= 0.8"
                elif conc_weight_res in ['FAIL', 'WARNING']:
                    pass_check = False
                    fail_reason = f"Conc Weight {conc_weight_res}"
                elif jpn_robust <= 0.8:
                    pass_check = False
                    fail_reason = f"JPN Robust {jpn_robust:.2f} <= 0.8"
            else:
                pass_check = False
                fail_reason = "No Check Data"

            if pass_check:
                # é€šè¿‡æ£€æŸ¥ -> æ”¾å…¥ passed_filter_alphas
                logger.info(f"âœ… Alpha {alpha_id} é€šè¿‡è¿‡æ»¤ (Fitness: {fitness:.2f})")
                state["passed_filter_alphas"].append({
                    "id": alpha_id,
                    "fitness": fitness
                })
                passed_count_this_run += 1
            else:
                logger.info(f"Alpha {alpha_id} æœªé€šè¿‡è¿‡æ»¤: {fail_reason}")
                
                # --- 3. ç”Ÿæˆåè½¬ä¿¡å· ---
                # æ¡ä»¶: sharpe < -1 å¹¶ä¸” fitness < -0.3
                if sharpe < -1.0 and fitness < -0.3 and expression:
                    logger.info(f"ğŸ’¡ Alpha {alpha_id} è§¦å‘åè½¬é€»è¾‘ (Sharpe: {sharpe:.2f}, Fitness: {fitness:.2f})")
                    try:
                        new_expression = f"reverse({expression})"
                        # ä¿æŒåŸæœ‰å‚æ•°è®¾ç½®
                        # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°è¿™ä¸ª alpha å¯¹åº”çš„åŸå§‹ ProposedAlpha é‡Œçš„è®¾ç½®
                        # è¿™æ˜¯ä¸€ä¸ªéš¾ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬è¿™é‡Œåªæœ‰ IDã€‚
                        # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨é»˜è®¤è®¾ç½®ï¼Œæˆ–è€…å°è¯•ä» alpha details è·å– (settings å­—æ®µ)
                        
                        # å°è¯•ä» API è¯¦æƒ…è·å–è®¾ç½®
                        settings_from_api = alpha_details.get("settings", {})
                        
                        # æ„é€ æ–°çš„ ProposedAlpha
                        new_proposal = ProposedAlpha(
                            reasoning=f"Generated from reversal of {alpha_id} (Sharpe {sharpe}, Fitness {fitness})",
                            new_expression=new_expression,
                            new_setting={
                                "neutralization": settings_from_api.get("neutralization", "INDUSTRY"),
                                "decay": settings_from_api.get("decay", 0),
                                "truncation": settings_from_api.get("truncation", 0.08)
                            }
                        )
                        
                        # æ”¾å…¥ valid_expressions ç­‰å¾…ä¸‹ä¸€è½®æ¨¡æ‹Ÿ
                        state["valid_expressions"].append(new_proposal)
                        logger.info(f"åè½¬ä¿¡å·å·²åŠ å…¥å¾…ä»¿çœŸé˜Ÿåˆ—: {new_expression}")
                        
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆåè½¬ä¿¡å·å¤±è´¥: {e}")

        # æ¸…ç©ºå½“å‰æ‰¹æ¬¡ ID
        state["current_batch_alpha_ids"] = []

    except Exception as e:
        logger.error(f"è¿‡æ»¤èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["error_log"].append(f"è¿‡æ»¤èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 4b ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state
    
    logger.info(f"{task_info} --- èŠ‚ç‚¹ 4b ç»“æŸ: æˆåŠŸ (æœ¬è½®é€šè¿‡ {passed_count_this_run} ä¸ª) ---")
    return state

def correlation_and_save(state: StageOneState) -> StageOneState:
    """
    èŠ‚ç‚¹ 4c: ç›¸å…³æ€§æ£€æŸ¥å¹¶ä¿å­˜
    èŒè´£ï¼šå¯¹æ‰€æœ‰é€šè¿‡è¿‡æ»¤çš„ alpha è¿›è¡Œç›¸å…³æ€§æ£€æŸ¥ï¼Œé€šè¿‡åˆ™å…¥åº“ã€‚
    """
    task_info = f"[Region: {state.get('region')} | Universe: {state.get('universe')} | Delay: {state.get('delay')} | Dataset: {state.get('dataset_ids')}]"
    logger.info(f"{task_info} --- å¼€å§‹èŠ‚ç‚¹ 4c: ç›¸å…³æ€§æ£€æŸ¥å¹¶ä¿å­˜ ---")
    state["iteration_count"] += 1
    
    try:
        session = check_session_and_relogin(state["session"])
        state["session"] = session
        
        passed_alphas = state.get("passed_filter_alphas", [])
        if not passed_alphas:
            logger.info("æ— å¾…æ£€æŸ¥ç›¸å…³æ€§çš„ Alphaã€‚")
            logger.info(f"{task_info} --- èŠ‚ç‚¹ 4c ç»“æŸ: è·³è¿‡ (æ—  Alpha) ---")
            return state

        # 1. å‡†å¤‡ç›¸å…³æ€§æ•°æ®
        logger.info("åŠ è½½ç›¸å…³æ€§å‚è€ƒæ•°æ®...")
        try:
            download_data(session, flag_increment=True)
            os_alpha_ids, os_alpha_rets = load_data()
            
            # --- Fix: Ensure no duplicate columns in os_alpha_rets ---
            if isinstance(os_alpha_rets, pd.DataFrame) and not os_alpha_rets.empty:
                if os_alpha_rets.columns.duplicated().any():
                    logger.warning("os_alpha_rets contains duplicate columns. Removing duplicates.")
                    os_alpha_rets = os_alpha_rets.loc[:, ~os_alpha_rets.columns.duplicated()]
            # ---------------------------------------------------------
            
        except Exception as e:
            logger.error(f"ç›¸å…³æ€§æ•°æ®åŠ è½½å¤±è´¥: {e}")
            os_alpha_ids, os_alpha_rets = {}, pd.DataFrame()

        region = state["region"]
        
        # 2. æ’åº (Fitness ä»å¤§åˆ°å°)
        passed_alphas.sort(key=lambda x: x['fitness'], reverse=True)
        
        logger.info(f"å¼€å§‹æ£€æŸ¥ {len(passed_alphas)} ä¸ª Alpha çš„ç›¸å…³æ€§...")
        
        dao = StageOneSignalDAO()
        new_saved_count = 0

        # 3. å¾ªç¯æ£€æŸ¥
        for alpha_item in passed_alphas:
            alpha_id = alpha_item['id']
            fitness = alpha_item['fitness']
            
            # è®¡ç®—ç›¸å…³æ€§
            try:
                max_corr, candidate_pnls_df = calc_self_corr(
                    alpha_id=alpha_id,
                    sess=session,
                    os_alpha_rets=os_alpha_rets,
                    os_alpha_ids=os_alpha_ids,
                    return_alpha_pnls=True
                )
            except Exception as e:
                logger.error(f"è®¡ç®—ç›¸å…³æ€§å‡ºé”™ {alpha_id}: {e}")
                max_corr = 1.0
                candidate_pnls_df = None

            prod_corr = 1.0
            if max_corr < 0.7:
                 prod_corr_result = safe_api_call(check_prod_corr_test, session, alpha_id=alpha_id)
                 prod_corr = prod_corr_result['value'].max() if prod_corr_result is not None and not prod_corr_result.empty else 1.0
            
            logger.info(f"Alpha {alpha_id} (Fit {fitness:.2f}): Self={max_corr:.2f}, Prod={prod_corr:.2f}")

            if max_corr < 0.7 and prod_corr < 0.7:
                # --- Pass: Save ---
                logger.info(f"âœ… Alpha {alpha_id} ç›¸å…³æ€§æ£€æŸ¥é€šè¿‡ï¼Œä¿å­˜ã€‚")
                
                signal_record = {
                    "alpha_id": alpha_id,
                    "region": state["region"],
                    "universe": state["universe"],
                    "delay": state["delay"],
                    "dataset_id": state["dataset_ids"][0] if state["dataset_ids"] else "unknown",
                    "category": state.get("category", "unknown"),
                    "date_time": state["date_time"],
                    "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                try:
                    dao.upsert_signal(signal_record)
                    if alpha_id not in state["saved_alphas_ids"]:
                        state["saved_alphas_ids"].append(alpha_id)
                    state["total_saved_count"] += 1
                    new_saved_count += 1

                    # Tag
                    tag = f"StageOne_{state.get('category', 'unknown')}_{state['date_time']}"
                    safe_api_call(set_alpha_properties, session, alpha_id, tags=[tag])

                except Exception as e:
                    logger.error(f"ä¿å­˜ Alpha {alpha_id} å¤±è´¥: {e}")
            else:
                logger.info(f"Alpha {alpha_id} ç›¸å…³æ€§è¿‡é«˜ï¼Œä¸¢å¼ƒã€‚")

        # æ¸…ç©º passed_filter_alphas
        state["passed_filter_alphas"] = []

    except Exception as e:
        logger.error(f"ç›¸å…³æ€§æ£€æŸ¥èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        state["error_log"].append(f"ç›¸å…³æ€§æ£€æŸ¥èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {e}")
        logger.info(f"{task_info} --- èŠ‚ç‚¹ 4c ç»“æŸ: å¼‚å¸¸å¤±è´¥ ---")
        return state

    logger.info(f"{task_info} --- èŠ‚ç‚¹ 4c ç»“æŸ: æˆåŠŸ (æ–°ä¿å­˜ {new_saved_count} ä¸ªï¼Œç´¯è®¡ {state['total_saved_count']} ä¸ª) ---")
    return state
# --- 4. å®šä¹‰æ¡ä»¶è¾¹ ---

def should_continue_simulation(state: StageOneState) -> str:
    """
    å†³å®šæ˜¯å¦ç»§ç»­ä»¿çœŸï¼ˆæ‰¹å¤„ç†ï¼‰
    é€»è¾‘ï¼šå¦‚æœ valid_expressions è¿˜æœ‰å‰©ä½™ï¼Œæˆ–è€…åˆšåˆšç”Ÿæˆäº†åè½¬ä¿¡å·æ”¾å…¥äº† valid_expressionsï¼Œåˆ™ç»§ç»­ä»¿çœŸ
    """
    valid_exprs = state.get("valid_expressions", [])
    if valid_exprs:
        logger.info(f"é˜Ÿåˆ—ä¸­ä»æœ‰ {len(valid_exprs)} ä¸ªå¾…ä»¿çœŸè¡¨è¾¾å¼ï¼Œç»§ç»­ä»¿çœŸå¾ªç¯ã€‚")
        return "continue_simulation"
    logger.info("æ‰€æœ‰å¾…ä»¿çœŸè¡¨è¾¾å¼å¤„ç†å®Œæ¯•ï¼Œè¿›å…¥ç›¸å…³æ€§æ£€æŸ¥ä¸ä¿å­˜é˜¶æ®µã€‚")
    return "go_to_save"

def should_continue_workflow(state: StageOneState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­æ•´ä¸ªå·¥ä½œæµè¿­ä»£"""
    if state.get("status") == "failed":
        return "end"
    
    # åœæ­¢æ¡ä»¶ 1: è¿ç»­ 3 æ¬¡æ— æœ‰æ•ˆå€™é€‰
    if state.get("consecutive_empty_candidates", 0) >= 3:
        logger.info("è¿ç»­ 3 æ¬¡æœªç”Ÿæˆæœ‰æ•ˆå€™é€‰ï¼Œåœæ­¢å·¥ä½œæµ")
        state["status"] = "completed"
        return "end"

    # åœæ­¢æ¡ä»¶ 2: ç›®æ ‡æ•°é‡è¾¾æˆ
    target_saved_count = 20
    if state.get("total_saved_count", 0) >= target_saved_count:
        logger.info(f"è¾¾åˆ°ç›®æ ‡ä¿å­˜æ•°é‡ {target_saved_count}ï¼Œå®Œæˆ")
        state["status"] = "completed"
        return "end"
        
    # åœæ­¢æ¡ä»¶ 3: å†å²å°è¯•è¿‡å¤š
    if len(state.get("historical_alphas", [])) > 2000:
        logger.info(f"å°è¯• Alpha è¶…è¿‡ 2000 ä¸ª (å«æ— æ•ˆæ•°æ® {state.get('insufficient_data_count', 0)})ï¼Œåœæ­¢")
        state["status"] = "completed"
        return "end"
        
    return "continue"

# --- 5. æ„å»ºå’Œè¿è¡Œå›¾ ---

def build_stage_one_graph():
    """æ„å»º LangGraph å·¥ä½œæµ"""
    workflow = StateGraph(StageOneState)
    
    workflow.add_node("initialize_and_load_data", initialize_and_load_data)
    workflow.add_node("generate_search_directions", generate_search_directions)
    workflow.add_node("generate_alpha_candidates", generate_alpha_candidates)
    workflow.add_node("validate_alpha_candidates", validate_alpha_candidates)
    
    # æ–°æ‹†åˆ†çš„èŠ‚ç‚¹
    workflow.add_node("simulate_alphas", simulate_alphas)
    workflow.add_node("filter_alphas", filter_alphas)
    workflow.add_node("correlation_and_save", correlation_and_save)
    
    workflow.set_entry_point("initialize_and_load_data")
    
    workflow.add_edge("initialize_and_load_data", "generate_search_directions")
    workflow.add_edge("generate_search_directions", "generate_alpha_candidates")
    workflow.add_edge("generate_alpha_candidates", "validate_alpha_candidates")
    workflow.add_edge("validate_alpha_candidates", "simulate_alphas")
    
    workflow.add_edge("simulate_alphas", "filter_alphas")
    
    # ä»¿çœŸ/è¿‡æ»¤å¾ªç¯ vs è¿›å…¥ä¿å­˜
    workflow.add_conditional_edges(
        "filter_alphas",
        should_continue_simulation,
        {
            "continue_simulation": "simulate_alphas",
            "go_to_save": "correlation_and_save"
        }
    )
    
    # ä¸»å¾ªç¯
    workflow.add_conditional_edges(
        "correlation_and_save",
        should_continue_workflow,
        {
            "end": END,
            "continue": "generate_search_directions" 
        }
    )
    
    return workflow.compile()

def run_stage_one_workflow(
    dataset_ids: List[str] = ["pv1"],
    category: str = "general",
    region: str = "USA",
    universe: str = "TOP3000",
    delay: int = 1,
    date_time: str = ""
) -> Dict[str, Any]:
    """è¿è¡Œ Stage One å·¥ä½œæµ"""
    logger.info("ğŸš€ å¯åŠ¨ Stage One å·¥ä½œæµ")
    
    try:
        session = start_session()
        if not session:
            return {"error": "ACE ä¼šè¯å¯åŠ¨å¤±è´¥"}
        
        # åˆå§‹åŒ–æ¨¡å‹
        if not initialize_model():
             return {"error": "LLM åˆå§‹åŒ–å¤±è´¥"}

        # åˆå§‹åŒ– date_time
        if not date_time:
            date_time = datetime.now().strftime("%Y%m%d")
        
        # æ¢å¤é€»è¾‘ï¼šæŸ¥æ‰¾å·²å­˜åœ¨çš„ Alphas
        dao = StageOneSignalDAO()
        existing_alphas = dao.get_alphas_by_batch(region, universe, delay, dataset_ids[0], date_time)
        
        historical_alphas = []
        saved_alphas_ids = []
        
        def _get_json_from_url(sess, url):
            """Helper to get and decode JSON from a URL within a safe_api_call."""
            response = sess.get(url)
            response.raise_for_status()
            return response.json()

        if existing_alphas:
            logger.info(f"å‘ç° {len(existing_alphas)} ä¸ªå·²å­˜åœ¨çš„ Alphaï¼Œæ­£åœ¨æ¢å¤ä¸Šä¸‹æ–‡...")
            for item in existing_alphas:
                alpha_id = item['alpha_id']
                saved_alphas_ids.append(alpha_id)
                
                # Fetch expression
                alpha_details = safe_api_call(_get_json_from_url, session, f"https://api.worldquantbrain.com/alphas/{alpha_id}")
                if alpha_details:
                    expression = alpha_details.get('regular', {}).get('code')
                    if expression:
                        historical_alphas.append(expression)
                    else:
                        logger.warning(f"æ¢å¤æ—¶æ— æ³•è·å– Alpha {alpha_id} çš„è¡¨è¾¾å¼")
                else:
                    logger.warning(f"æ¢å¤æ—¶æ— æ³•è·å– Alpha {alpha_id} çš„è¯¦æƒ…")
            
            logger.info(f"æˆåŠŸæ¢å¤ {len(historical_alphas)} ä¸ªå†å²è¡¨è¾¾å¼")

        app = build_stage_one_graph()
        
        initial_state = {
            "session": session,
            "dataset_ids": dataset_ids,
            "category": category,
            "region": region,
            "universe": universe,
            "delay": delay,
            "date_time": date_time, 
            "valid_data_fields": None,
            "search_directions": [],
            "generated_expressions": [],
            "valid_expressions": [],
            "current_batch_alpha_ids": [],
            "passed_filter_alphas": [],
            "historical_alphas": historical_alphas,
            "repeat_historical_alphas": [],
            "saved_alphas_ids": saved_alphas_ids,
            "iteration_count": 0,
            "total_saved_count": len(saved_alphas_ids),
            "insufficient_data_count": 0,
            "consecutive_empty_candidates": 0,
            "status": "running",
            "error_log": []
        }
        
        final_state = app.invoke(initial_state)
        
        # ä¿å­˜å·¥ä½œæµçŠ¶æ€ç»“æœ
        try:
            output_dir = "output/workflow_state_result"
            os.makedirs(output_dir, exist_ok=True)
            
            # åˆ›å»ºå¯åºåˆ—åŒ–çš„çŠ¶æ€å‰¯æœ¬
            serializable_state = {}
            for k, v in final_state.items():
                if k == "session":
                    continue
                # ä¸éœ€è¦ä¿å­˜ search_directions å’Œ generated_expressions
                if k in ["search_directions", "generated_expressions"]:
                    continue
                    
                if k == "valid_data_fields":
                    # å¦‚æœæ˜¯ DataFrameï¼Œåªè®°å½•æ•°é‡æˆ–è½¬æ¢ä¸ºå­—å…¸
                    if isinstance(v, pd.DataFrame):
                        serializable_state[k] = f"DataFrame with {len(v)} rows"
                    else:
                        serializable_state[k] = str(v)
                elif k == "valid_expressions": # ä¿æŒ valid_expressions çš„å¤„ç† (è™½ç„¶ generated_expressions ç§»é™¤äº†ï¼Œä½† valid ä¹Ÿè®¸æœ‰ç”¨ï¼Œä¸è¿‡ç”¨æˆ·æ²¡æ˜ç¡®è¯´ç§»é™¤ validï¼Œåªè¯´äº† generated)
                    # å¤„ç† Pydantic æ¨¡å‹åˆ—è¡¨
                    serializable_state[k] = [item.model_dump() if hasattr(item, "model_dump") else item for item in v]
                else:
                    serializable_state[k] = v
            
            file_name = f"{category}_{region}_{date_time}_{int(time.time())}.json"
            file_path = os.path.join(output_dir, file_name)
            
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(serializable_state, f, indent=2, ensure_ascii=False)
            logger.info(f"âœ… å·¥ä½œæµæœ€ç»ˆçŠ¶æ€å·²ä¿å­˜è‡³: {file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")

        return final_state
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµè¿è¡Œå¼‚å¸¸: {e}")
        return {"success": False, "error": str(e)}

def main():
    # å¯ä»¥åœ¨æ­¤å¤„ä¿®æ”¹è¿è¡Œå‚æ•°
    #dataset_ids = ["other534", "shortinterest3", "shortinterest38", "shortinterest5", "shortinterest55"]
    #category = "shortinterest"
    dataset_ids = ["fundamental28"]
    category = "fundamental"
    region = "ASI"
    universe = "MINVOL1M"
    delay = 1
    date_time = "" # ç•™ç©ºåˆ™è‡ªåŠ¨ä½¿ç”¨å½“å‰æ—¥æœŸ

    
    session = start_session()

    """
    df = get_datafields_with_cache(
                    session,
                    region=region,
                    delay=delay,
                    universe=universe,
                    dataset_id="analyst10",
                    data_type="ALL"
                )
    datafields_str = df[['id', 'description', 'type', 'coverage']].to_string(index=False) if isinstance(df, pd.DataFrame) else str(df)
    
    print(datafields_str)
    """
    operators_df = safe_api_call(get_operators, session)
    regular_operators_df = operators_df[operators_df['scope'] == 'REGULAR']
    stage_one_operators_df = regular_operators_df[regular_operators_df['name'].isin(ALLOWED_OPERATORS)]

    print(stage_one_operators_df.to_string(
    max_cols=None,      # æ˜¾ç¤ºæ‰€æœ‰åˆ—
    max_colwidth=300,   # æ¯åˆ—æœ€å¤§å®½åº¦
    show_dimensions=True  # æ˜¾ç¤ºç»´åº¦ä¿¡æ¯
    ))

    """
    print(operators_df.to_string(
    max_cols=None,      # æ˜¾ç¤ºæ‰€æœ‰åˆ—
    max_colwidth=100,   # æ¯åˆ—æœ€å¤§å®½åº¦
    show_dimensions=True  # æ˜¾ç¤ºç»´åº¦ä¿¡æ¯
    ))

    result = run_stage_one_workflow(
        dataset_ids=dataset_ids,
        category=category,
        region=region,
        universe=universe,
        delay=delay,
        date_time=date_time
    )

    if result.get("status") != "failed":
        print(f"âœ… å·¥ä½œæµè¿è¡ŒæˆåŠŸï¼Œä¿å­˜æ•°é‡: {result.get('total_saved_count')}")
    else:
        print(f"âŒ å·¥ä½œæµè¿è¡Œç»“æŸï¼ŒçŠ¶æ€: {result.get('status')}")
        if result.get("error_log"):
            print(f"é”™è¯¯æ—¥å¿—: {result.get('error_log')}")
    """

if __name__ == "__main__":
    main()
