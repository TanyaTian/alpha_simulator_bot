import queue
import time
import csv
import requests
import os
import ast
import json
import threading
from pytz import timezone
from logger import Logger  
from datetime import datetime, timedelta
from signal_manager import SignalManager

# è·å–ç¾å›½ä¸œéƒ¨æ—¶é—´
eastern = timezone('US/Eastern')
fmt = '%Y-%m-%d'
loc_dt = datetime.now(eastern)
print("Current time in Eastern is", loc_dt.strftime(fmt))

class AlphaSimulator:
    """Alphaæ¨¡æ‹Ÿå™¨ç±»ï¼Œç”¨äºç®¡ç†é‡åŒ–ç­–ç•¥çš„æ¨¡æ‹Ÿè¿‡ç¨‹"""

    TIMESTAMP_FORMAT = "%Y%m%d_%H%M%S"
    FILE_CONFIG = {
        "input_file": "alpha_list_pending_simulated.csv",
        "fail_file": "fail_alphas.csv",
        "output_file": "simulated_alphas.csv",  # åŸºç¡€æ–‡ä»¶å
        "state_file": "simulator_state.json"
    }

    def __init__(self, max_concurrent, username, password, batch_number_for_every_queue, batch_size=10, signal_manager=None):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨

        Args:
            max_concurrent (int): æœ€å¤§å¹¶å‘æ¨¡æ‹Ÿæ•°é‡
            username (str): ç™»å½•ç”¨æˆ·å
            password (str): ç™»å½•å¯†ç 
            batch_number_for_every_queue (int): æ¯æ‰¹å¤„ç†æ•°é‡
        """
        self.running = True

        # åˆ›å»º Logger å®ä¾‹
        self.logger = Logger()

        # æ³¨å†Œä¿¡å·å¤„ç†
        if signal_manager:
            signal_manager.add_handler(self.signal_handler)
        else:
            self.logger.warning("æœªæä¾› SignalManagerï¼ŒAlphaSimulator æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å‡½æ•°")

        # æ„å»ºåŸºç¡€è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        self.data_dir = os.path.join(project_root, 'data')

        # è‡ªåŠ¨åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        os.makedirs(self.data_dir, exist_ok=True)

        # æ„å»ºæ–‡ä»¶è·¯å¾„ä½“ç³»
        self.alpha_list_file_path = os.path.join(self.data_dir, self.FILE_CONFIG["input_file"])
        self.fail_alphas = os.path.join(self.data_dir, self.FILE_CONFIG["fail_file"])
        self.state_file = os.path.join(self.data_dir, self.FILE_CONFIG["state_file"])

        # å…³é”®æ–‡ä»¶å­˜åœ¨æ€§éªŒè¯
        self._validate_critical_files()

        # å…¶ä»–å±æ€§åˆå§‹åŒ–
        self.max_concurrent = max_concurrent
        self.active_simulations = []
        self.active_update_time = time.time()
        self.username = username
        self.password = password
        self.session = self.sign_in(username, password)
        self.sim_queue_ls = []
        self.batch_number_for_every_queue = batch_number_for_every_queue
        self.batch_size = batch_size  # å­˜å‚¨ batch_size ä½œä¸ºå®ä¾‹å˜é‡
        self.lock = threading.Lock()  # ğŸ”’ æ–‡ä»¶å†™å…¥é”
        self.task_queue = queue.Queue()

        # å¯åŠ¨ worker çº¿ç¨‹
        try:
            self.worker_thread = threading.Thread(target=self.worker)
            self.worker_thread.daemon = True
            self.worker_thread.start()
            self.logger.info("Worker thread started")
        except Exception as e:
            self.logger.error(f"Failed to start worker thread: {e}")
            raise
        
        # å¯åŠ¨æ–‡ä»¶è½®è½¬çº¿ç¨‹
        try:
            self.start_rotation_scheduler()
        except Exception as e:
            self.logger.error(f"Failed to start file rotation thread: {e}")
            raise

        # åŠ è½½ä¸Šæ¬¡æœªå®Œæˆçš„ active_simulations
        self._load_previous_state()

    def signal_handler(self, signum, frame):
        self.logger.info(f"Received shutdown signal {signum}, , initiating shutdown...")
        self.running = False
        self.save_state()
        self.shutdown()

    def _validate_critical_files(self):
        """éªŒè¯å…³é”®è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        missing_files = []

        if not os.path.exists(self.alpha_list_file_path):
            missing_files.append(
                f"ä¸»è¾“å…¥æ–‡ä»¶: {self.alpha_list_file_path}\n"
                "å¯èƒ½åŸå› :\n"
                "- æ–‡ä»¶å°šæœªç”Ÿæˆ\n"
                "- æ–‡ä»¶åæ‹¼å†™é”™è¯¯"
            )

        if missing_files:
            raise FileNotFoundError(
                "å…³é”®æ–‡ä»¶ç¼ºå¤±:\n\n" +
                "\n\n".join(missing_files) +
                "\n\nè§£å†³æ–¹æ¡ˆå»ºè®®:\n"
                "1. æ£€æŸ¥dataç›®å½•ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸ\n"
                "2. ç¡®è®¤æ–‡ä»¶ç”Ÿæˆæµç¨‹å·²æ‰§è¡Œ"
            )

    def _load_previous_state(self):
        if os.path.exists(self.state_file):
            with open(self.state_file, 'r') as f:
                state = json.load(f)
                self.active_simulations = state.get("active_simulations", [])
                self.logger.info(f"Loaded {len(self.active_simulations)} previous active simulations from state.")

    def rotate_output_file(self):
        """æ¯å¤© 0 ç‚¹è½®è½¬ output æ–‡ä»¶ï¼Œå°†å‰ä¸€å¤©çš„æ–‡ä»¶é‡å‘½åä¸º YYYY-MM-DD æ ¼å¼"""
        current_date = datetime.now().date()
        yesterday = current_date - timedelta(days=1)
        yesterday_str = yesterday.strftime('%Y-%m-%d')
        output_file = os.path.join(self.data_dir, self.FILE_CONFIG["output_file"])
        backup_file = os.path.join(self.data_dir, f'simulated_alphas.csv.{yesterday_str}')

        # ä½¿ç”¨æ–‡ä»¶é”ä¿æŠ¤æ–‡ä»¶æ“ä½œ
        with self.lock:
            if os.path.exists(output_file):
                if os.path.exists(backup_file):
                    os.remove(backup_file)  # åˆ é™¤æ—§å¤‡ä»½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                os.rename(output_file, backup_file)
                self.logger.info(f"Rotated {output_file} to {backup_file}")
            else:
                self.logger.warning(f"Output file {output_file} does not exist for rotation")

    def start_rotation_scheduler(self):
        """å¯åŠ¨æ–‡ä»¶è½®è½¬çº¿ç¨‹ï¼Œæ¯ååˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡"""
        def rotation_check_loop():
            # è®°å½•ä¸Šä¸€æ¬¡è½®è½¬çš„æ—¥æœŸ
            last_rotation_date = datetime.now().date()
            self.logger.info("File rotation thread started")
            
            while True:
                # è·å–å½“å‰æ—¥æœŸ
                current_date = datetime.now().date()
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ–°çš„ä¸€å¤©
                if current_date > last_rotation_date:
                    self.logger.info(f"New date detected: {current_date}, starting file rotation")
                    self.rotate_output_file()
                    # æ›´æ–°æœ€åè½®è½¬æ—¥æœŸ
                    last_rotation_date = current_date
                
                # è®°å½•ä¼‘çœ å¼€å§‹
                self.logger.debug("File rotation check completed, sleeping for 10 minutes")
                # ä¼‘çœ 10åˆ†é’Ÿï¼ˆ600ç§’ï¼‰
                time.sleep(600)
        
        # å¯åŠ¨è½®è½¬çº¿ç¨‹ï¼Œè®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹
        rotation_thread = threading.Thread(target=rotation_check_loop, daemon=True)
        rotation_thread.start()
        self.logger.info("Output file rotation thread started, checking every 10 minutes")

    def sign_in(self, username, password):
        """ç™»å½•WorldQuant BRAINå¹³å°"""
        s = requests.Session()
        s.auth = (username, password)
        count = 0
        count_limit = 30

        while True:
            try:
                response = s.post('https://api.worldquantbrain.com/authentication')
                response.raise_for_status()
                break
            except:
                count += 1
                self.logger.error("Connection down, trying to login again...")
                time.sleep(15)
                if count > count_limit:
                    self.logger.error(f"{username} failed too many times, returning None.")
                    return None
        self.logger.info("Login to BRAIN successfully.")
        return s

    def manage_input_files(self):
        """ç®¡ç†è¾“å…¥æ–‡ä»¶ï¼Œå½“ä¸»æ–‡ä»¶ä¸ºç©ºæ—¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç¼–å·çš„æ–‡ä»¶"""
        base_name = self.FILE_CONFIG["input_file"].replace('.csv', '')
        current_file = self.alpha_list_file_path
        
        # æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦ä¸ºç©ºï¼ˆåªæœ‰è¡¨å¤´æˆ–å®Œå…¨ä¸ºç©ºï¼‰
        try:
            with open(current_file, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)  # è¯»å–è¡¨å¤´
                if header is None:  # å®Œå…¨ç©ºæ–‡ä»¶
                    self.logger.info("Current input file is completely empty")
                else:
                    first_data_row = next(reader, None)  # å°è¯•è¯»å–ç¬¬ä¸€è¡Œæ•°æ®
                    if first_data_row is not None:  # æœ‰æ•°æ®è¡Œ
                        return False  # æ–‡ä»¶ä¸ä¸ºç©ºï¼Œä¸éœ€è¦åˆ‡æ¢
                    self.logger.info("Current input file has only header but no data")
        except FileNotFoundError:
            self.logger.info("Current input file not found")
            pass  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œä¸‹é¢çš„é€»è¾‘
        
        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªå¯ç”¨çš„æ–‡ä»¶
        for i in range(1, 20):
            next_file = os.path.join(self.data_dir, f"{base_name}_{i}.csv")
            if os.path.exists(next_file):
                # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦ä¸ºç©ºï¼ˆåªæœ‰è¡¨å¤´ï¼‰
                with open(next_file, 'r') as f:
                    reader = csv.reader(f)
                    header = next(reader, None)
                    if header is None:  # å®Œå…¨ç©ºæ–‡ä»¶
                        self.logger.info(f"Found {base_name}_{i}.csv but it's completely empty")
                        continue
                    first_data_row = next(reader, None)
                    if first_data_row is None:  # åªæœ‰è¡¨å¤´
                        self.logger.info(f"Found {base_name}_{i}.csv but it has only header")
                        continue
                
                try:
                    # åˆ é™¤å½“å‰ç©ºæ–‡ä»¶
                    if os.path.exists(current_file):
                        os.remove(current_file)
                    # é‡å‘½åä¸‹ä¸€ä¸ªæ–‡ä»¶ä¸ºå½“å‰æ–‡ä»¶
                    os.rename(next_file, current_file)
                    self.logger.info(f"Switched input file to {base_name}_{i}.csv")
                    return True
                except OSError as e:
                    self.logger.error(f"Error switching input files: {e}")
                    return False
        
        # æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€ä¸ªæ–‡ä»¶
        self.logger.info("No more valid input files available")
        return False

    def read_alphas_from_csv_in_batches(self, batch_size=50):
        """ä»CSVæ–‡ä»¶ä¸­åˆ†æ‰¹è¯»å–alphas"""
        self.logger.debug(f"Starting to read alphas in batches (batch_size={batch_size})")
        alphas = []
        temp_file_name = self.alpha_list_file_path + '.tmp'
        self.logger.debug(f"Using temporary file: {temp_file_name}")

        # æ£€æŸ¥å¹¶ç®¡ç†è¾“å…¥æ–‡ä»¶
        if not os.path.exists(self.alpha_list_file_path):
            if not self.manage_input_files():
                return alphas  # æ²¡æœ‰æ›´å¤šæ–‡ä»¶å¯ç”¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
        else:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åªæœ‰è¡¨å¤´
            with open(self.alpha_list_file_path, 'r') as f:
                reader = csv.reader(f)
                header = next(reader, None)
                if header is None:  # å®Œå…¨ç©ºæ–‡ä»¶
                    if not self.manage_input_files():
                        return alphas
                else:
                    first_data_row = next(reader, None)
                    if first_data_row is None:  # åªæœ‰è¡¨å¤´
                        if not self.manage_input_files():
                            return alphas

        with open(self.alpha_list_file_path, 'r') as file, open(temp_file_name, 'w', newline='') as temp_file:
            reader = csv.DictReader(file)
            fieldnames = reader.fieldnames
            self.logger.debug(f"CSV fieldnames: {fieldnames}")
            if fieldnames is None:  # å®Œå…¨ç©ºæ–‡ä»¶æƒ…å†µ
                self.logger.warning("Input file has no headers/fieldnames")
                return alphas
                
            # ç¡®ä¿è¡¨å¤´åŒ…å«é¢„æœŸå­—æ®µ
            expected_fields = {'type', 'settings', 'regular'}
            if not expected_fields.issubset(set(fieldnames)):
                self.logger.error(f"Input file missing required fields. Expected: {expected_fields}, found: {fieldnames}")
                return alphas
                
            writer = csv.DictWriter(temp_file, fieldnames=fieldnames)
            writer.writeheader()
            self.logger.debug(f"Initialized CSV writer with fieldnames: {fieldnames}")

            self.logger.debug(f"Reading {batch_size} alphas from file")
            read_count = 0
            for _ in range(batch_size):
                try:
                    row = next(reader)
                    read_count += 1
                    if 'settings' in row:
                        if isinstance(row['settings'], str):
                            try:
                                row['settings'] = ast.literal_eval(row['settings'])
                            except (ValueError, SyntaxError):
                                self.logger.error(f"Error evaluating settings: {row['settings']}")
                        elif not isinstance(row['settings'], dict):
                            self.logger.error(f"Unexpected type for settings: {type(row['settings'])}")
                    alphas.append(row)
                except StopIteration:
                    self.logger.debug(f"Reached end of file after reading {read_count} alphas")
                    break

            remaining_rows = []
            for row in reader:
                if None in row:
                    self.logger.error(f"Row contains None key: {row}")
                    continue
                if not all(field in row for field in fieldnames):
                    self.logger.error(f"Row missing fields: {row} (expected: {fieldnames})")
                    continue
                self.logger.debug(f"Valid row: {row}")
                remaining_rows.append(row)
            
            for row in remaining_rows:
                try:
                    writer.writerow(row)
                except ValueError as e:
                    self.logger.error(f"Failed to write row: {row}. Error: {e}")
                    raise

        try:
            os.replace(temp_file_name, self.alpha_list_file_path)
            self.logger.info(f"Successfully replaced {self.alpha_list_file_path} with new data")
        except OSError as e:
            if hasattr(e, 'winerror') and e.winerror == 5:  # æ‹’ç»è®¿é—®
                self.logger.error("Access denied when replacing file. Trying to delete original and rename temporary.")
                try:
                    os.remove(self.alpha_list_file_path)
                    os.rename(temp_file_name, self.alpha_list_file_path)
                    self.logger.info("Successfully handled file replacement after access denied")
                except OSError as e2:
                    self.logger.error(f"Failed to delete original or rename temporary file: {e2}")
            else:
                self.logger.error(f"Unexpected error during file replacement: {e}")
                raise

        queue_path = os.path.join(self.data_dir, 'sim_queue.csv')
        if alphas:
            self.logger.debug(f"Writing {len(alphas)} alphas to queue file: {queue_path}")
            with open(queue_path, 'w', newline='') as file:
                writer = csv.DictWriter(file, fieldnames=alphas[0].keys())
                if file.tell() == 0:
                    writer.writeheader()
                writer.writerows(alphas)
            self.logger.info(f"Successfully wrote batch of {len(alphas)} alphas to queue")
        else:
            self.logger.warning("No alphas were read in this batch")
        return alphas
    def simulate_alpha(self, alpha_list):
        """
        æ¨¡æ‹Ÿä¸€ç»„ alpha è¡¨è¾¾å¼ï¼Œé€šè¿‡ API æäº¤æ‰¹é‡æ¨¡æ‹Ÿè¯·æ±‚ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰ã€‚

        Args:
            alpha_list (list): åŒ…å«å¤šä¸ª alpha æ•°æ®çš„åˆ—è¡¨ï¼Œæ¯ä¸ª alpha æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« typeã€settings å’Œ regular å­—æ®µ
                - type: å­—ç¬¦ä¸²ï¼Œæ¨¡æ‹Ÿç±»å‹ï¼Œä¾‹å¦‚ "REGULAR"
                - settings: å­—å…¸ï¼Œæ¨¡æ‹Ÿè®¾ç½®ï¼Œä¾‹å¦‚ {'instrumentType': 'EQUITY', ...}
                - regular: å­—ç¬¦ä¸²ï¼Œalpha è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ "ts_quantile(winsorize(...), 22)"

        Returns:
            str or None: æ¨¡æ‹Ÿè¿›åº¦ URLï¼ˆlocation_urlï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        # å°† alpha_list è½¬æ¢ä¸º sim_data_listï¼Œç”¨äº API è¯·æ±‚
        sim_data_list = self.generate_sim_data(alpha_list)

        # å¦‚æœ sim_data_list ä¸ºç©ºï¼ˆä¾‹å¦‚ alpha_list ä¸­æ‰€æœ‰ alpha éƒ½æ— æ•ˆï¼‰ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›
        if not sim_data_list:
            self.logger.error("No valid simulation data generated from alpha_list")
            # å°†æ•´ä¸ª alpha_list å†™å…¥å¤±è´¥æ–‡ä»¶
            with open(self.fail_alphas, 'a', newline='') as file:
                writer = csv.DictWriter(file, fieldnames=alpha_list[0].keys())
                for alpha in alpha_list:
                    writer.writerow(alpha)
            return None

        # åˆå§‹åŒ–é‡è¯•è®¡æ•°å™¨
        count = 0

        # é‡è¯•å¾ªç¯ï¼Œæœ€å¤šå°è¯• 35 æ¬¡
        while True:
            try:
                # ä½¿ç”¨ self.session å‘é€ POST è¯·æ±‚åˆ° WorldQuant Brain å¹³å°çš„ API
                response = self.session.post('https://api.worldquantbrain.com/simulations', json=sim_data_list)

                # æ£€æŸ¥ HTTP çŠ¶æ€ç ï¼Œå¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚ 4xx æˆ– 5xxï¼‰ï¼ŒæŠ›å‡ºå¼‚å¸¸
                response.raise_for_status()

                # æ£€æŸ¥å“åº”å¤´ä¸­æ˜¯å¦åŒ…å« Location å­—æ®µ
                if "Location" in response.headers:
                    # å¦‚æœæˆåŠŸè·å– Locationï¼Œè®°å½•æ—¥å¿—å¹¶è¿”å›
                    self.logger.info("Alpha batch location retrieved successfully.")
                    self.logger.info(f"Location: {response.headers['Location']}")
                    return response.headers['Location']

            except requests.exceptions.RequestException as e:
                # æ•è· HTTP è¯·æ±‚ç›¸å…³çš„å¼‚å¸¸ï¼ˆä¾‹å¦‚ç½‘ç»œé”™è¯¯ã€æœåŠ¡å™¨é”™è¯¯ï¼‰
                self.logger.error(f"Error in sending simulation request: {e}")

                # å¦‚æœé‡è¯•æ¬¡æ•°è¶…è¿‡ 35 æ¬¡ï¼Œé‡æ–°ç™»å½•å¹¶è·³å‡ºå¾ªç¯
                if count > 35:
                    # è°ƒç”¨ sign_in æ–¹æ³•é‡æ–°ç™»å½•
                    self.session = self.sign_in(self.username, self.password)
                    self.logger.error("Error occurred too many times, skipping this alpha batch and re-logging in.")
                    break

                # è®°å½•é‡è¯•ä¿¡æ¯ï¼Œç­‰å¾… 5 ç§’åç»§ç»­
                self.logger.error("Error in sending simulation request. Retrying after 5s...")
                time.sleep(5)
                count += 1

        # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ˆé‡è¯•æ¬¡æ•°è€—å°½ï¼‰ï¼Œè®°å½•é”™è¯¯
        self.logger.error(f"Simulation request failed after {count} attempts for alpha batch")
        # å°†æ•´ä¸ª alpha_list å†™å…¥å¤±è´¥æ–‡ä»¶
        with open(self.fail_alphas, 'a', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=alpha_list[0].keys())
            for alpha in alpha_list:
                writer.writerow(alpha)
        return None
    
    def generate_sim_data(self, alpha_list):
        """
        å°† alpha_list è½¬æ¢ä¸º sim_data_listï¼Œç”¨äºæ‰¹é‡æ¨¡æ‹Ÿã€‚

        Args:
            alpha_list (list): åŒ…å«å¤šä¸ª alpha å­—å…¸çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« typeã€settings å’Œ regular å­—æ®µ
                - type: å­—ç¬¦ä¸²ï¼Œæ¨¡æ‹Ÿç±»å‹ï¼Œä¾‹å¦‚ "REGULAR"
                - settings: å­—å…¸ï¼Œæ¨¡æ‹Ÿè®¾ç½®ï¼Œä¾‹å¦‚ {'instrumentType': 'EQUITY', ...}
                - regular: å­—ç¬¦ä¸²ï¼Œalpha è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ "ts_quantile(winsorize(...), 22)"

        Returns:
            list: åŒ…å«å¤šä¸ª simulation_data å­—å…¸çš„åˆ—è¡¨ï¼Œæ ¼å¼ç¬¦åˆ API è¦æ±‚
        """
        sim_data_list = []

        for alpha in alpha_list:
            # ç¡®ä¿ alpha åŒ…å«å¿…è¦çš„å­—æ®µ
            if not all(key in alpha for key in ['type', 'settings', 'regular']):
                self.logger.error(f"Invalid alpha data, missing required fields: {alpha}")
                continue

            # æ£€æŸ¥ settings æ˜¯å¦ä¸ºå­—å…¸ï¼ˆç†è®ºä¸Šæ€»æ˜¯å­—å…¸ï¼‰
            settings = alpha['settings']
            if not isinstance(settings, dict):
                self.logger.error(f"Unexpected type for settings: {type(settings)}. Expected dict. Skipping this alpha.")
                continue

            # æ„é€  simulation_data å­—å…¸
            simulation_data = {
                'type': alpha['type'],
                'settings': settings,
                'regular': alpha['regular']
            }

            sim_data_list.append(simulation_data)

        self.logger.info(f"Generated sim_data_list with {len(sim_data_list)} entries")
        return sim_data_list
        
    def load_new_alpha_and_simulate(self):
        """
        åŠ è½½å¹¶æ¨¡æ‹Ÿæ–°çš„ alphaï¼Œæ¯æ¬¡ä»é˜Ÿåˆ—ä¸­å¼¹å‡º self.batch_size ä¸ª alpha è¿›è¡Œæ‰¹é‡æ¨¡æ‹Ÿã€‚
        ç¡®ä¿ alpha_list ä¸­çš„æ‰€æœ‰ alpha å…·æœ‰ç›¸åŒçš„ 'region' å’Œ 'universe' è®¾ç½®ã€‚

        Attributes:
            self.sim_queue_ls (list): åŒ…å« alpha æ•°æ®çš„é˜Ÿåˆ—ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« typeã€settings å’Œ regular å­—æ®µ
            self.batch_size (int): æ¯æ¬¡æ¨¡æ‹Ÿçš„ alpha æ‰¹é‡å¤§å°
            self.max_concurrent (int): æœ€å¤§å¹¶å‘æ¨¡æ‹Ÿæ•°é‡
            self.active_simulations (list): æ­£åœ¨è¿›è¡Œçš„æ¨¡æ‹Ÿä»»åŠ¡åˆ—è¡¨ï¼Œå­˜å‚¨ location_url
            self.active_update_time (float): æœ€è¿‘ä¸€æ¬¡æ¨¡æ‹Ÿæ›´æ–°çš„æ—¶é—´æˆ³
            self.logger (Logger): æ—¥å¿—è®°å½•å™¨
        """
        # æ£€æŸ¥è¿è¡ŒçŠ¶æ€ï¼Œå¦‚æœ running ä¸º Falseï¼Œåˆ™é€€å‡º
        if not self.running:
            return

        # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œä» CSV æ–‡ä»¶ä¸­è¯»å–æ–°çš„ alpha æ•°æ®
        if len(self.sim_queue_ls) < 1:
            self.logger.info("Simulation queue is empty, reading new alphas from CSV...")
            self.sim_queue_ls = self.read_alphas_from_csv_in_batches(self.batch_number_for_every_queue)

        # å¦‚æœå½“å‰æ­£åœ¨è¿›è¡Œçš„æ¨¡æ‹Ÿä»»åŠ¡æ•°é‡è¾¾åˆ°æœ€å¤§å¹¶å‘é™åˆ¶ï¼Œç­‰å¾… 2 ç§’åé€€å‡º
        if len(self.active_simulations) >= self.max_concurrent:
            self.logger.info(f"Max concurrent simulations reached ({self.max_concurrent}). Waiting 2 seconds")
            time.sleep(2)
            return

        self.logger.info('Loading new alphas for simulation...')
        try:
            # æ¯æ¬¡ä»é˜Ÿåˆ—ä¸­å¼¹å‡º alphaï¼Œç»„æˆ alpha_list
            alpha_list = []

            # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œç›´æ¥è¿”å›
            if not self.sim_queue_ls:
                self.logger.info("No alphas available in the queue.")
                return

            # å¼¹å‡ºç¬¬ä¸€ä¸ª alphaï¼Œè®¾ç½®åŸºå‡† region å’Œ universe
            first_alpha = self.sim_queue_ls.pop(0)
            alpha_list.append(first_alpha)
            base_region = first_alpha['settings'].get('region')
            base_universe = first_alpha['settings'].get('universe')

            # éªŒè¯ç¬¬ä¸€ä¸ª alpha çš„ region å’Œ universe æ˜¯å¦å­˜åœ¨
            if base_region is None or base_universe is None:
                self.logger.error(f"First alpha missing 'region' or 'universe': {first_alpha}")
                return

            self.logger.info(f"Base region: {base_region}, Base universe: {base_universe}")

            # ç»§ç»­å¼¹å‡º alphaï¼Œç›´åˆ°è¾¾åˆ° batch_size æˆ–é‡åˆ°ä¸ä¸€è‡´çš„ region/universe
            while len(alpha_list) < self.batch_size and self.sim_queue_ls:
                # æ£€æŸ¥ä¸‹ä¸€ä¸ª alpha çš„ region å’Œ universe
                next_alpha = self.sim_queue_ls[0]  # æŸ¥çœ‹ä½†ä¸å¼¹å‡º
                next_region = next_alpha['settings'].get('region')
                next_universe = next_alpha['settings'].get('universe')

                # éªŒè¯ region å’Œ universe æ˜¯å¦å­˜åœ¨
                if next_region is None or next_universe is None:
                    self.logger.error(f"Alpha missing 'region' or 'universe': {next_alpha}")
                    self.sim_queue_ls.pop(0)  # ç§»é™¤æ— æ•ˆ alpha
                    continue

                # æ£€æŸ¥æ˜¯å¦ä¸åŸºå‡†ä¸€è‡´
                if next_region != base_region or next_universe != base_universe:
                    self.logger.info(
                        f"Region/Universe mismatch: {next_region}/{next_universe} does not match base {base_region}/{base_universe}. Stopping batch.")
                    break

                # å¦‚æœä¸€è‡´ï¼Œå¼¹å‡ºå¹¶æ·»åŠ åˆ° alpha_list
                alpha_list.append(self.sim_queue_ls.pop(0))

            # å¦‚æœæˆåŠŸå¼¹å‡º alpha åˆ—è¡¨ï¼Œæ‰§è¡Œæ¨¡æ‹Ÿ
            if alpha_list:
                # è®°å½•å½“å‰æ‰¹æ¬¡çš„ alpha ä¿¡æ¯
                self.logger.info(f"Starting simulation for {len(alpha_list)} alphas:")
                for alpha in alpha_list:
                    self.logger.info(f"  - Alpha: {alpha['regular'][:50]}... with settings: {alpha['settings']}")
                self.logger.info(f"Remaining in sim_queue_ls: {len(self.sim_queue_ls)}")

                # è°ƒç”¨ simulate_alphaï¼Œä¼ å…¥ alpha åˆ—è¡¨
                location_url = self.simulate_alpha(alpha_list)

                # å¦‚æœæ¨¡æ‹ŸæˆåŠŸï¼ˆè¿”å› location_urlï¼‰ï¼Œå°† URL æ·»åŠ åˆ° active_simulations
                if location_url:
                    self.active_simulations.append(location_url)
                    self.active_update_time = time.time()
                    self.logger.info(f"Simulation started, location_url: {location_url}")
                else:
                    self.logger.warning("Simulation failed, no location_url returned")
            else:
                self.logger.info("No alphas available for simulation in this batch")

        except IndexError:
            # å¦‚æœé˜Ÿåˆ—ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ alphaï¼Œæ‰“å°ä¿¡æ¯
            self.logger.info("No more alphas available in the queue.")
        except Exception as e:
            # æ•è·å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
            self.logger.error(f"Error during simulation: {e}")
    
    def check_simulation_progress(self, simulation_progress_url):
        """
        æ£€æŸ¥æ‰¹é‡æ¨¡æ‹Ÿçš„è¿›åº¦ï¼Œè·å– children åˆ—è¡¨ã€‚

        Args:
            simulation_progress_url (str): æ‰¹é‡æ¨¡æ‹Ÿçš„è¿›åº¦ URLï¼Œä¾‹å¦‚ "https://api.worldquantbrain.com/simulations/12345"

        Returns:
            list or None: å¦‚æœæˆåŠŸè¿”å› children åˆ—è¡¨ï¼Œå¦åˆ™è¿”å› None
        """
        try:
            simulation_progress = self.session.get(simulation_progress_url)
            simulation_progress.raise_for_status()

            # æ£€æŸ¥æ˜¯å¦åŒ…å« Retry-After å¤´ï¼Œè¡¨ç¤ºæœåŠ¡ç«¯æš‚æ—¶ä¸å¯ç”¨
            if simulation_progress.headers.get("Retry-After", 0) != 0:
                return None

            # è§£æå“åº”ï¼Œæå– children å’ŒçŠ¶æ€
            progress_data = simulation_progress.json()
            children = progress_data.get("children", [])
            if not children:
                self.logger.error("No children found in simulation progress response.")
                return None

            status = progress_data.get("status")
            self.logger.info(f"Simulation batch status: {status}, children count: {len(children)}")

            if status != "COMPLETE":
                self.logger.info("Simulation not complete. Will check again later.")

            return children

        except requests.exceptions.HTTPError as e:
            remove_status_codes = {400, 403, 404, 410}
            if e.response.status_code in remove_status_codes:
                self.logger.error(f"Simulation request failed with status {e.response.status_code}: {e}")
                # Remove the simulation_progress_url from active_simulations
                if hasattr(self, 'active_simulations') and simulation_progress_url in self.active_simulations:
                    self.active_simulations.remove(simulation_progress_url)
                    self.logger.info(f"Removed {simulation_progress_url} from active_simulations due to status code {e.response.status_code}")
                return None
            else:
                self.logger.error(f"Failed to fetch simulation progress: {e}")
                self.session = self.sign_in(self.username, self.password)
                return None
        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to fetch simulation progress: {e}")
            self.session = self.sign_in(self.username, self.password)
            return None


    def process_children_async(self, children):
        """å¼‚æ­¥æ”¾å…¥ children å¤„ç†ä»»åŠ¡"""
        if not children:
            self.logger.warning("Empty children list received. Skipping enqueue.")
            return

        try:
            self.task_queue.put(children, timeout=5)
            self.logger.info(f"Enqueued task for {len(children)} children. Queue size: {self.task_queue.qsize()}")
        except queue.Full:
            self.logger.error(f"Task queue is full (maxsize=100). Dropping task for {len(children)} children.")
            temp_file = os.path.join(self.data_dir, f"dropped_children_{int(time.time())}.json")
            with open(temp_file, 'w') as f:
                json.dump(children, f)
            self.logger.info(f"Saved dropped children to {temp_file}")


    def process_children(self, children):
        """åŒæ­¥å¤„ç† childrenï¼Œè·å– alphaId å’Œ alpha detailï¼Œå†™å…¥æ–‡ä»¶"""
        self.logger.info(f"Processing {len(children)} children...")
        if not children:
            self.logger.warning("Empty children list received. Skipping processing.")
            return

        brain_api_url = "https://api.worldquantbrain.com"
        alpha_details_list = []
        max_retries = 3
        retry_delay = 5
        request_timeout = 30

        for child in children:
            self.logger.debug(f"Processing child: {child}")
            if not isinstance(child, str):
                self.logger.error(f"Invalid child format: {child} (Expected string)")
                continue

            for attempt in range(max_retries):
                try:
                    self.logger.debug(f"Fetching child simulation progress: {child} (Attempt {attempt + 1}/{max_retries})")
                    child_progress = self.session.get(f"{brain_api_url}/simulations/{child}", timeout=request_timeout)
                    child_progress.raise_for_status()
                    child_data = child_progress.json()
                    self.logger.debug(f"Child response: {child_data}")

                    alpha_id = child_data.get("alpha")
                    if not alpha_id:
                        self.logger.warning(f"No alpha_id found for child: {child}. Response: {child_data}")
                        break

                    self.logger.debug(f"Fetching alpha detail for alpha_id: {alpha_id}")
                    alpha_response = self.session.get(f"{brain_api_url}/alphas/{alpha_id}", timeout=request_timeout)
                    alpha_response.raise_for_status()
                    alpha_details = alpha_response.json()
                    if not alpha_details:
                        self.logger.warning(f"Empty alpha details for alpha_id: {alpha_id}")
                        break
                    # è½¬æ¢æ•°æ®æ ¼å¼
                    formatted = {
                        "id": alpha_details.get("id"),
                        "type": alpha_details.get("type"),
                        "author": alpha_details.get("author"),
                        "settings": str(alpha_details.get("settings", {})),
                        "regular": str(alpha_details.get("regular", {})),
                        "dateCreated": alpha_details.get("dateCreated"),
                        "dateSubmitted": alpha_details.get("dateSubmitted"),
                        "dateModified": alpha_details.get("dateModified"),
                        "name": alpha_details.get("name"),
                        "favorite": alpha_details.get("favorite", False),
                        "hidden": alpha_details.get("hidden", False),
                        "color": alpha_details.get("color"),
                        "category": alpha_details.get("category"),
                        "tags": str(alpha_details.get("tags", [])),
                        "classifications": str(alpha_details.get("classifications", [])),
                        "grade": alpha_details.get("grade"),
                        "stage": alpha_details.get("stage"),
                        "status": alpha_details.get("status"),
                        "is": str(alpha_details.get("is", {})),
                        "os": alpha_details.get("os"),
                        "train": alpha_details.get("train"),
                        "test": alpha_details.get("test"),
                        "prod": alpha_details.get("prod"),
                        "competitions": alpha_details.get("competitions"),
                        "themes": str(alpha_details.get("themes", [])),
                        "pyramids": str(alpha_details.get("pyramids", [])),
                        "team": alpha_details.get("team")
                    }

                    alpha_details_list.append(formatted)
                    self.logger.debug(f"Successfully retrieved alpha details for alpha_id: {alpha_id}")
                    break

                except requests.exceptions.RequestException as e:
                    self.logger.error(f"Error fetching child {child} or alpha detail (Attempt {attempt + 1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        self.logger.info(f"Retrying after {retry_delay}s...")
                        time.sleep(retry_delay)
                    else:
                        self.logger.error(f"Max retries reached for child {child}. Skipping...")
                        if "401" in str(e) or "403" in str(e):
                            self.logger.info("Authentication error detected. Re-signing in...")
                            self.session = self.sign_in(self.username, self.password)
                        break
                except ValueError as e:
                    self.logger.error(f"Invalid JSON response for child {child}: {e}")
                    break
                except Exception as e:
                    self.logger.error(f"Unexpected error processing child {child}: {e}")
                    break

        if alpha_details_list:
            output_file = os.path.join(self.data_dir, self.FILE_CONFIG["output_file"])
            try:
                with self.lock:
                    self.logger.info(f"Writing {len(alpha_details_list)} alpha details to file: {output_file}")
                    if not os.access(self.data_dir, os.W_OK):
                        self.logger.error(f"No write permission for directory: {self.data_dir}")
                        return
                    fieldnames = alpha_details_list[0].keys()
                    with open(output_file, 'a', newline='') as file:
                        writer = csv.DictWriter(file, fieldnames=fieldnames)
                        if os.stat(output_file).st_size == 0:
                            writer.writeheader()
                        for alpha_details in alpha_details_list:
                            try:
                                writer.writerow(alpha_details)
                                self.logger.debug(f"Wrote alpha details: {alpha_details.get('id', 'unknown')}")
                            except Exception as e:
                                self.logger.error(f"Error writing alpha details {alpha_details.get('id', 'unknown')}: {e}")
                    self.logger.info(f"Successfully wrote {len(alpha_details_list)} alpha details to {output_file}")
            except PermissionError as e:
                self.logger.error(f"Permission error writing to file {output_file}: {e}")
            except Exception as e:
                self.logger.error(f"Error writing to file {output_file}: {e}")
        else:
            self.logger.warning(f"No alpha details to write for {len(children)} children. alpha_details_list is empty.")

    def worker(self):
        """åå°å­çº¿ç¨‹ï¼Œå¤„ç†é˜Ÿåˆ—ä¸­çš„ children åˆ—è¡¨"""
        self.logger.info("Worker thread started. Waiting for tasks...")
        while self.running:
            try:
                children = self.task_queue.get(timeout=10)
                if children is None:
                    self.logger.info("Shutdown signal received. Worker thread exiting.")
                    self.task_queue.task_done()
                    break

                self.logger.info(f"Worker received a task with {len(children)} children.")
                self.process_children(children)
                self.logger.info("Worker finished processing current task.")
                self.task_queue.task_done()

            except queue.Empty:
                self.logger.debug("Task queue is empty. Waiting for new tasks...")
                continue
            except Exception as e:
                self.logger.error(f"Unexpected error in worker thread: {e}")
                self.task_queue.task_done()
                continue

        self.logger.info("Worker thread stopped.")

    def shutdown(self):
        """ä¼˜é›…å…³é—­å­çº¿ç¨‹"""
        self.logger.info("Shutting down the worker thread...")
        self.task_queue.put(None)
        if self.worker_thread.is_alive():
            self.worker_thread.join(timeout=10)
            if self.worker_thread.is_alive():
                self.logger.warning("Worker thread did not terminate within 10 seconds.")
        self.logger.info("Worker thread has been successfully shut down.")

    def check_simulation_status(self):
        """æ£€æŸ¥æ‰€æœ‰æ´»è·ƒæ¨¡æ‹Ÿçš„çŠ¶æ€"""
        current_time = time.time()
        time_diff = current_time - self.active_update_time
        if time_diff > 3600 and len(self.active_simulations) > 0:
            self.logger.warning(f"active_update_time exceeds 1 hour (diff: {time_diff:.2f} seconds). Resetting active simulations.")
            self.session = self.sign_in(self.username, self.password)
            self.active_simulations.clear()
            self.logger.info("Active simulations cleared after re-signing in.")

        if not self.active_simulations:
            self.logger.info("No active simulations to check.")
            return

        self.logger.info(f"Checking {len(self.active_simulations)} active simulations...")
        count = 0
        for sim_url in self.active_simulations[:]:
            children = self.check_simulation_progress(sim_url)
            if children is None:
                count += 1
                self.logger.debug(f"Simulation {sim_url} still in progress or failed.")
                continue

            self.logger.info(f"Simulation batch {sim_url} completed. Removing from active list.")
            self.active_simulations.remove(sim_url)
            self.process_children_async(children)
            self.active_update_time = time.time()

        self.logger.info(f"Total {count} simulations still in progress for account {self.username}.")

    def save_state(self):
        """ä¿å­˜å½“å‰çŠ¶æ€"""
        state = {
            "active_simulations": self.active_simulations,
            "timestamp": datetime.now(eastern).strftime(self.TIMESTAMP_FORMAT)
        }
        with open(self.state_file, 'w') as f:
            json.dump(state, f)
        self.logger.info(f"Saved {len(self.active_simulations)} active simulations to {self.state_file}")

        if self.sim_queue_ls:
            with open(self.alpha_list_file_path, 'a', newline='') as file:
                writer = csv.DictWriter(file, fieldnames=self.sim_queue_ls[0].keys())
                if os.stat(self.alpha_list_file_path).st_size == 0:
                    writer.writeheader()
                writer.writerows(self.sim_queue_ls)
            self.logger.info(f"Reinserted {len(self.sim_queue_ls)} alphas back to {self.alpha_list_file_path}")
            self.sim_queue_ls = []

    def manage_simulations(self):
        """ç®¡ç†æ•´ä¸ªæ¨¡æ‹Ÿè¿‡ç¨‹"""
        if not self.session:
            self.logger.error("Failed to sign in. Exiting...")
            return

        try:
            while self.running:
                self.check_simulation_status()
                self.load_new_alpha_and_simulate()
                time.sleep(3)
        except KeyboardInterrupt:
            self.logger.info("Manual interruption detected.")
