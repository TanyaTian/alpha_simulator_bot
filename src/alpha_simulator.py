import time
import os
import ast
import json
import threading
import random
import pandas as pd
from typing import List, Dict, Optional, Union, Tuple
from collections import deque
from datetime import datetime

from logger import Logger
from dao import SimulationTasksDAO, AlphaListPendingSimulatedDAO, StageOneSignalDAO
from config_manager import config_manager
from cache_manager import CacheManager
from utils import safe_api_call
from ace_lib import (
    check_session_and_relogin,
    simulate_multi_alpha, get_alpha_yearly_stats, get_check_submission,
    check_prod_corr_test, set_alpha_properties
)
from self_corr_calculator import calc_self_corr, load_data, download_data

from datetime import datetime

class TaskTracker:
    """ç”¨äºè·Ÿè¸ªä»»åŠ¡æ‰§è¡Œæ—¶é—´å¹¶åœ¨è¶…æ—¶æ—¶é‡Šæ”¾ä¿¡å·é‡"""
    def __init__(self, semaphore, timeout, logger):
        self.semaphore = semaphore
        self.timeout = timeout
        self.logger = logger
        self.start_time = time.time()
        self.released = False
        self.timed_out = False
        self.lock = threading.Lock()

    def release(self, timed_out=False):
        with self.lock:
            if not self.released:
                self.semaphore.release()
                self.released = True
                if timed_out:
                    self.timed_out = True
                return True
            return False

    def check_timeout(self):
        if self.timed_out:
            return True
        if time.time() - self.start_time > self.timeout:
            if self.release(timed_out=True):
                self.logger.warning(f"Task timed out after {self.timeout}s")
            return True
        return False

class AlphaSimulator:
    """Alphaæ¨¡æ‹Ÿå™¨ç±»ï¼Œç”¨äºç®¡ç†é‡åŒ–ç­–ç•¥çš„æ¨¡æ‹Ÿè¿‡ç¨‹"""

    def __init__(self, signal_manager=None):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿå™¨"""
        self.running = True
        self.logger = Logger()

        # æ³¨å†Œä¿¡å·å¤„ç†
        if signal_manager:
            signal_manager.add_handler(self.signal_handler)
        else:
            self.logger.warning("æœªæä¾› SignalManager, AlphaSimulator æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å‡½æ•°")

        # ä»é…ç½®ä¸­å¿ƒè·å–å‚æ•°
        self._load_config_from_manager()
        
        # æ³¨å†Œé…ç½®è§‚å¯Ÿè€…
        self._config_observer_handle = config_manager.on_config_change(self._handle_config_change)
        
        # åˆå§‹åŒ–DAO
        self.alpha_list_pending_simulated_dao = AlphaListPendingSimulatedDAO()
        self.simulation_task_dao = SimulationTasksDAO()
        self.stage_one_signal_dao = StageOneSignalDAO()

        # è®¡æ•°å™¨
        self.total_sent_count = 0

        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = CacheManager(
            alpha_dao=self.alpha_list_pending_simulated_dao,
            task_dao=self.simulation_task_dao,
            batch_number_for_every_queue=self.batch_number_for_every_queue
        )
        
        # å¹¶å‘æ§åˆ¶
        self.active_tasks = []
        self.lock = threading.Lock()
        self.semaphore = threading.Semaphore(self.max_concurrent)

    def signal_handler(self, signum, frame):
        self.logger.info(f"Received shutdown signal {signum}, initiating shutdown...")
        self.running = False
        self.cache_manager.flush()

    def _load_config_from_manager(self):
        """ä»é…ç½®ä¸­å¿ƒåŠ è½½è¿è¡Œæ—¶å‚æ•°"""
        config = config_manager._config
        self.max_concurrent = config.get('max_concurrent', 5)
        self.batch_number_for_every_queue = config.get('batch_number_for_every_queue', 100)
        self.batch_size = config.get('batch_size', 10)
        
        region_set = config.get('region_set', ['USA'])
        if not isinstance(region_set, list):
            region_set = ['USA']
        self.region_set = deque(region_set)
        
        self.session = config_manager.get_session()

    def _handle_config_change(self, new_config):
        """é…ç½®å˜æ›´å›è°ƒå¤„ç†"""
        self._load_config_from_manager()
        self.logger.info("Configuration reloaded due to config center update")

    def simulate_alphas(self, alpha_list: List[Dict]) -> List[Dict]:
        """
        æ¨¡æ‹Ÿä¸€ç»„ alphaã€‚
        å‚è€ƒ stage_one_workflow.py çš„ simulate_alphas èŠ‚ç‚¹é€»è¾‘ã€‚
        """
        sim_jobs = []
        for alpha in alpha_list:
            sim_jobs.append({
                "type": alpha.get('type', 'REGULAR'),
                "settings": alpha.get('settings', {}),
                "regular": alpha.get('regular', '')
            })
        
        try:
            session = check_session_and_relogin(self.session)
            self.session = session
            # æ‰§è¡Œæ‰¹é‡ä»¿çœŸ
            return safe_api_call(simulate_multi_alpha, session, sim_jobs)
        except Exception as e:
            self.logger.error(f"simulate_alphas failed: {e}")
            return []

    def filter_alphas(self, alpha_ids: List[str], tracker: TaskTracker = None) -> Tuple[List[Dict], List[Dict]]:
        """
        è¿‡æ»¤ç¬¦åˆæ¡ä»¶çš„ alphaï¼Œå¹¶ç”Ÿæˆå¯èƒ½çš„åè½¬ä¿¡å·ã€‚
        å‚è€ƒ stage_one_workflow.py çš„ filter_alphas èŠ‚ç‚¹é€»è¾‘ã€‚
        """
        passed_alphas = []
        reversal_proposals = []
        session = check_session_and_relogin(self.session)
        self.session = session
        
        for alpha_id in alpha_ids:
            if tracker and tracker.timed_out:
                break
            try:
                # è·å–è¡¨è¾¾å¼ (ç”¨äºè®°å½•å†å²å’Œç”Ÿæˆåè½¬ä¿¡å·)
                expression = ""
                alpha_details = {}
                try:
                    response = session.get(f"https://api.worldquantbrain.com/alphas/{alpha_id}")
                    response.raise_for_status()
                    alpha_details = response.json()
                    expression = alpha_details.get('regular', {}).get('code', "")
                except Exception as e:
                    self.logger.error(f"è·å– Alpha {alpha_id} è¯¦æƒ…å¤±è´¥: {e}")

                # è·å–ç»Ÿè®¡å’Œæ£€æŸ¥æ•°æ®
                stats_df = safe_api_call(get_alpha_yearly_stats, session, alpha_id)
                check_df = safe_api_call(get_check_submission, session, alpha_id)
                
                # 1. Year State æ£€æŸ¥ (Data History)
                active_years = 0
                if stats_df is not None and not stats_df.empty:
                    active_years = (stats_df['sharpe'] != 0.0).sum()
                
                if active_years < 8:
                    continue
                
                # 2. Check ä¿¡æ¯æ£€æŸ¥
                if check_df is not None and not check_df.empty:
                    checks = check_df.set_index('name')['value'].to_dict()
                    checks_result = check_df.set_index('name')['result'].to_dict()
                    
                    sharpe = checks.get('LOW_SHARPE', 0)
                    fitness = checks.get('LOW_FITNESS', 0)
                    robust = checks.get('LOW_ROBUST_UNIVERSE_SHARPE', 1.0)
                    jpn_robust = checks.get('LOW_ASI_JPN_SHARPE', 1.0)
                    conc_weight_res = checks_result.get('CONCENTRATED_WEIGHT', 'PASS')
                    
                    pass_check = True
                    fail_reason = ""
                    
                    if sharpe <= 1.0:
                        pass_check = False
                        fail_reason = f"Sharpe {sharpe:.2f} <= 1.0"
                    elif fitness <= 0.3:
                        pass_check = False
                        fail_reason = f"Fitness {fitness:.2f} <= 0.3"
                    elif robust <= 0.8:
                        pass_check = False
                        fail_reason = f"Robust {robust:.2f} <= 0.8"
                    elif conc_weight_res in ['FAIL', 'WARNING']:
                        pass_check = False
                        fail_reason = f"Conc Weight {conc_weight_res}"
                    elif jpn_robust <= 0.8:
                        pass_check = False
                        fail_reason = f"JPN Robust {jpn_robust:.2f} <= 0.8"

                    if pass_check:
                        self.logger.info(f"âœ… Alpha {alpha_id} é€šè¿‡è¿‡æ»¤ (Fitness: {fitness:.2f})")
                        passed_alphas.append({
                            'id': alpha_id,
                            'fitness': fitness
                        })
                    else:
                        self.logger.info(f"Alpha {alpha_id} æœªé€šè¿‡è¿‡æ»¤: {fail_reason}")
                        # 3. ç”Ÿæˆåè½¬ä¿¡å·
                        # æ¡ä»¶: sharpe < -1 å¹¶ä¸” fitness < -0.3
                        if sharpe < -1.0 and fitness < -0.3 and expression:
                            self.logger.info(f"ğŸ’¡ Alpha {alpha_id} è§¦å‘åè½¬é€»è¾‘ (Sharpe: {sharpe:.2f}, Fitness: {fitness:.2f})")
                            try:
                                settings_from_api = alpha_details.get("settings", {})
                                reversal_proposals.append({
                                    'type': 'REGULAR',
                                    'settings': settings_from_api,
                                    'regular': f"reverse({expression})"
                                })
                            except Exception as e:
                                self.logger.error(f"ç”Ÿæˆåè½¬ä¿¡å·å¤±è´¥: {e}")
            except Exception as e:
                self.logger.error(f"Error filtering alpha {alpha_id}: {e}")
                
        return passed_alphas, reversal_proposals

    def correlation_and_save(self, passed_alphas: List[Dict], context: Dict):
        """
        ç›¸å…³æ€§æ£€æŸ¥å¹¶ä¿å­˜ã€‚
        å‚è€ƒ stage_one_workflow.py çš„ correlation_and_save èŠ‚ç‚¹é€»è¾‘ã€‚
        """
        if not passed_alphas:
            return

        session = check_session_and_relogin(self.session)
        self.session = session
        
        try:
            # åŠ è½½ç›¸å…³æ€§å‚è€ƒæ•°æ®
            download_data(session, flag_increment=True)
            os_alpha_ids, os_alpha_rets = load_data()
            if isinstance(os_alpha_rets, pd.DataFrame) and not os_alpha_rets.empty:
                if os_alpha_rets.columns.duplicated().any():
                    os_alpha_rets = os_alpha_rets.loc[:, ~os_alpha_rets.columns.duplicated()]
        except Exception as e:
            self.logger.error(f"Failed to load correlation data: {e}")
            os_alpha_ids, os_alpha_rets = {}, pd.DataFrame()

        # æ’åº (Fitness ä»å¤§åˆ°å°)
        passed_alphas.sort(key=lambda x: x['fitness'], reverse=True)

        for alpha_item in passed_alphas:
            alpha_id = alpha_item['id']
            try:
                # è®¡ç®—è‡ªç›¸å…³æ€§
                max_corr = calc_self_corr(alpha_id, session, os_alpha_rets, os_alpha_ids)
                
                prod_corr = 1.0
                if max_corr < 0.7:
                    # æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒç›¸å…³æ€§
                    prod_corr_result = safe_api_call(check_prod_corr_test, session, alpha_id)
                    prod_corr = prod_corr_result['value'].max() if prod_corr_result is not None and not prod_corr_result.empty else 1.0
                
                if max_corr < 0.7 and prod_corr < 0.7:
                    # é€šè¿‡æ£€æŸ¥ï¼Œä¿å­˜ä¿¡å·
                    record = {
                        "alpha_id": alpha_id,
                        "region": context['region'],
                        "universe": context['universe'],
                        "delay": context['delay'],
                        "date_time": context['date_time'],
                        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    self.stage_one_signal_dao.upsert_signal(record)
                    # æ‰“æ ‡ç­¾
                    safe_api_call(set_alpha_properties, session, alpha_id, tags=[f"AlphaSim_{context['date_time']}"])
                    self.logger.info(f"âœ… Saved Alpha {alpha_id} (Fitness: {alpha_item['fitness']:.2f})")
            except Exception as e:
                self.logger.error(f"Error in correlation/save for {alpha_id}: {e}")

    def workflow_slot(self, region: str, tracker: TaskTracker):
        """
        å•ä¸ªå·¥ä½œæµå¡æ§½çš„æ‰§è¡Œæµç¨‹ã€‚
        """
        try:
            self.logger.info(f"ğŸš€ [Workflow Slot] Started for region: {region}")
            
            # 1. ä»æ•°æ®åº“æå– batch_size ä¸ª pending alpha
            self.logger.info(f"ğŸ“‹ [Stage 1/4] Fetching {self.batch_size} pending alphas from database for {region}...")
            # ç›´æ¥ä»æ•°æ®åº“è·å–å¹¶é”å®š
            db_records = self.alpha_list_pending_simulated_dao.fetch_and_lock_pending_by_region(region, self.batch_size)
            if not db_records:
                self.logger.info(f"â„¹ï¸ No pending alphas found for region: {region}. Slot exiting.")
                return

            # 2. å°†è¿™æ‰¹ alpha çš„çŠ¶æ€æ”¹ä¸º sent
            self.logger.info(f"ğŸ”„ [Stage 1/4] Marking {len(db_records)} records as 'sent'...")
            record_ids = [r['id'] for r in db_records]
            # ç›´æ¥ä½¿ç”¨ DAO æ›´æ–°çŠ¶æ€
            self.alpha_list_pending_simulated_dao.batch_update_status_by_ids(record_ids, 'sent')
            
            # ç´¯åŠ  total_sent_count
            with self.lock:
                self.total_sent_count += len(record_ids)
                self.logger.info(f"ğŸ“ˆ Total sent count updated: {self.total_sent_count}")

            self.logger.info(f"ğŸ—ï¸ [Stage 2/4] Generating simulation job list...")
            alpha_list = []
            for record in db_records:
                self.logger.debug(f"Processing DB record: {record['id']}")
                settings = record.get('settings')
                raw_settings_for_log = settings # Keep a copy for logging
                
                if isinstance(settings, str):
                    try:
                        # Try json.loads first as it's more common for stored JSON
                        settings = json.loads(settings)
                    except json.JSONDecodeError:
                        try:
                            # Fallback to ast.literal_eval if it's a Python literal string
                            settings = ast.literal_eval(settings)
                        except Exception as e:
                            self.logger.error(f"âŒ Failed to parse settings for record {record['id']}. "
                                             f"Value: {raw_settings_for_log}, Type: {type(raw_settings_for_log)}, Error: {e}")
                            settings = {}
                elif settings is None:
                    self.logger.warning(f"âš ï¸ Settings is None for record {record['id']}")
                    settings = {}
                elif not isinstance(settings, dict):
                    self.logger.warning(f"âš ï¸ Settings is neither string nor dict for record {record['id']}. "
                                       f"Type: {type(settings)}, Value: {settings}")
                    settings = {}
                
                alpha_item = {
                    'type': record.get('type', 'REGULAR'),
                    'settings': settings,
                    'regular': record.get('regular', '')
                }
                alpha_list.append(alpha_item)
            
            self.logger.info(f"âœ… [Stage 2/4] Generated {len(alpha_list)} initial jobs.")

            all_passed = []
            # 3. å¾ªç¯æ‰§è¡Œ simulate_alphas å’Œ filter_alphasï¼Œç›´åˆ° batch size ä¸ªå…¨éƒ¨æ¨¡æ‹Ÿå®Œ
            self.logger.info(f"ğŸ“¡ [Stage 3/4] Starting simulation and filtering cycle (Sub-batches of 10)...")
            idx = 0
            sub_batch_count = 0
            while idx < len(alpha_list):
                if tracker.timed_out:
                    self.logger.warning(f"âš ï¸ [Stage 3/4] Workflow slot for region {region} ABORTED due to timeout during simulation.")
                    break
                
                sub_batch_count += 1
                batch = alpha_list[idx:idx+10]
                idx += 10
                
                self.logger.info(f"ğŸ§ª [Sub-batch {sub_batch_count}] Simulating {len(batch)} alphas...")
                sim_results = self.simulate_alphas(batch)
                
                # æå– alpha id
                ids = [r['alpha_id'] for r in sim_results if r.get('alpha_id')]
                self.logger.info(f"ğŸ” [Sub-batch {sub_batch_count}] Received {len(ids)} Alpha IDs. Starting filtering...")
                
                if ids:
                    passed, reversals = self.filter_alphas(ids, tracker=tracker)
                    all_passed.extend(passed)
                    self.logger.info(f"âœ¨ [Sub-batch {sub_batch_count}] {len(passed)} passed filter. {len(reversals)} reversals generated.")
                    
                    if reversals:
                        self.logger.info(f"â• Adding {len(reversals)} reversal proposals to the queue.")
                        alpha_list.extend(reversals)
            
            # 4. å°†æ‰€æœ‰é€šè¿‡ filter_alphas çš„ alpha id ä¼ å…¥ correlation_and_save
            if tracker.timed_out:
                self.logger.warning(f"âš ï¸ [Stage 4/4] Skipping correlation and save for {region} due to timeout.")
            elif all_passed:
                self.logger.info(f"ğŸ“Š [Stage 4/4] Starting correlation check and save for {len(all_passed)} passed alphas...")
                first_settings = alpha_list[0]['settings']
                context = {
                    'region': region,
                    'universe': first_settings.get('universe', 'TOP3000'),
                    'delay': first_settings.get('delay', 1),
                    'date_time': datetime.now().strftime("%Y%m%d")
                }
                self.correlation_and_save(all_passed, context)
            else:
                self.logger.info(f"â„¹ï¸ [Stage 4/4] No alphas passed filtering for {region}. Skipping correlation.")

            self.logger.info(f"ğŸ [Workflow Slot] Finished for region: {region}")

        except Exception as e:
            self.logger.error(f"âŒ Error in workflow_slot for region {region}: {e}", exc_info=True)
        finally:
            # 5. å®Œæˆåé‡Šæ”¾å¡æ§½
            tracker.release()

    def _check_timeouts(self):
        """æ£€æŸ¥æ‰€æœ‰æ´»åŠ¨ä»»åŠ¡çš„è¶…æ—¶æƒ…å†µ"""
        with self.lock:
            remaining_tasks = []
            for thread, tracker, region in self.active_tasks:
                if not thread.is_alive():
                    # ç¡®ä¿ä¿¡å·é‡å·²é‡Šæ”¾ï¼ˆå¦‚æœçº¿ç¨‹æ­£å¸¸ç»“æŸï¼‰
                    tracker.release()
                    continue
                
                if tracker.check_timeout():
                    # å·²è¶…æ—¶ï¼Œtracker.check_timeout ä¼šå¤„ç†ä¿¡å·é‡é‡Šæ”¾
                    # æˆ‘ä»¬åœæ­¢è·Ÿè¸ªå®ƒï¼Œè®©çº¿ç¨‹åœ¨ä¸‹æ¬¡å¾ªç¯æ£€æŸ¥ timed_out æ—¶é€€å‡º
                    continue
                
                remaining_tasks.append((thread, tracker, region))
            self.active_tasks = remaining_tasks

    def manage_simulations(self):
        """ç®¡ç†æ•´ä¸ªæ¨¡æ‹Ÿè¿‡ç¨‹"""
        if not self.session:
            self.logger.error("No session available for AlphaSimulator. Exiting...")
            return

        self.logger.info("AlphaSimulator starting simulation management...")
        while self.running:
            # 0. Check limit
            is_limit_reached = False
            with self.lock:
                if self.total_sent_count >= self.batch_number_for_every_queue:
                    is_limit_reached = True
            
            if is_limit_reached:
                self.logger.warning(f"ğŸ›‘ Limit reached ({self.total_sent_count} >= {self.batch_number_for_every_queue}). Sleeping for 1 hour...")
                
                # Sleep for 1 hour, but check self.running every second
                sleep_start = time.time()
                while time.time() - sleep_start < 3600:
                    if not self.running:
                        break
                    time.sleep(1)
                
                if not self.running:
                    break
                    
                continue

            # 1. æ£€æŸ¥è¶…æ—¶
            self._check_timeouts()
            
            # 2. åˆ¤æ–­ max_concurrentï¼Œå¦‚æœè¿˜æ²¡æœ‰è¾¾åˆ°é…ç½®ä¸Šé™
            if self.semaphore.acquire(timeout=1):
                if not self.running:
                    self.semaphore.release()
                    break
                
                with self.lock:
                    if not self.region_set:
                        self.semaphore.release()
                        time.sleep(5)
                        continue
                    region = self.region_set[0]
                    self.region_set.rotate(-1)
                
                # è®¡ç®—è¶…æ—¶æ—¶é—´ï¼šbatch_size / 10 * 30 åˆ†é’Ÿ
                timeout_minutes = (self.batch_size / 10) * 30
                timeout_seconds = timeout_minutes * 60
                
                # åˆ›å»º Tracker å’Œ çº¿ç¨‹
                tracker = TaskTracker(self.semaphore, timeout_seconds, self.logger)
                t = threading.Thread(target=self.workflow_slot, args=(region, tracker))
                t.daemon = True
                t.start()
                
                with self.lock:
                    self.active_tasks.append((t, tracker, region))
            else:
                # å¡æ§½å·²æ»¡ï¼Œç­‰å¾…
                time.sleep(3)

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æ¸…ç†"""
        if hasattr(self, '_config_observer_handle'):
            try:
                config_manager._observers.remove(self._handle_config_change)
            except:
                pass
        if hasattr(self, 'cache_manager'):
            self.cache_manager.flush()
