import configparser
import os
import csv
import pandas as pd
import ast
import re
import time
import random
import json
from datetime import datetime
from pytz import timezone
from logger import Logger
from typing import List, Tuple, Optional
from requests.exceptions import ConnectionError
from http.client import RemoteDisconnected
import ace_lib as ace
from cached_data_fetcher import get_datafields_with_cache

# åˆ›å»ºå…¨å±€ Logger å®žä¾‹
logger = Logger()

def get_eastern_time():
    """èŽ·å–ç¾Žä¸œæ—¶é—´"""
    eastern = timezone("US/Eastern")
    return datetime.now(eastern)

def is_midnight_eastern():
    """æ£€æŸ¥æ˜¯å¦ä¸ºç¾Žä¸œæ—¶é—´ 0 ç‚¹"""
    eastern_time = get_eastern_time()
    return eastern_time.hour == 0 and eastern_time.minute == 0

def read_csv(file_path):
    """è¯»å– CSV æ–‡ä»¶ä¸º DataFrame"""
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    return pd.DataFrame()

def write_csv(df, file_path):
    """å†™å…¥ DataFrame åˆ° CSV æ–‡ä»¶"""
    df.to_csv(file_path, index=False)


def load_config(config_file="config/config.ini"):
    """
    ä»Žé…ç½®æ–‡ä»¶è¯»å– usernameã€passwordã€max_concurrent å’Œ batch_number_for_every_queue, batch_sizeã€‚

    Args:
        config_file (str): é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º "config/config.ini"ã€‚

    Returns:
        dict: åŒ…å« username, password, max_concurrent, batch_number_for_every_queue, batch_size çš„å­—å…¸ï¼Œ
              å¦‚æžœè¯»å–å¤±è´¥åˆ™è¿”å›ž Noneã€‚
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    config_path = os.path.join(project_root, config_file)

    config = configparser.ConfigParser()

    try:
        if not os.path.exists(config_path):
            logger.error(f"Config file {config_path} not found.")
            return None

        config.read(config_path)
        
        # Parse region_set as a list
        region_set_str = config.get('Credentials', 'region_set', fallback='US')
        try:
            region_set = ast.literal_eval(region_set_str)
            if not isinstance(region_set, list):
                raise ValueError(f"Expected list for region_set, got {type(region_set)}")
        except (ValueError, SyntaxError) as e:
            logger.error(f"Error parsing region_set: {e}. Using default ['US']")
            region_set = ['US']

        config_data = {
            'username': config.get('Credentials', 'username'),
            'password': config.get('Credentials', 'password'),
            'max_concurrent': config.getint('Credentials', 'max_concurrent'),
            'batch_number_for_every_queue': config.getint('Credentials', 'batch_number_for_every_queue'),
            'batch_size': config.getint('Credentials', 'batch_size'),
            'init_date_str': config.get('Credentials', 'init_date_str'),
            'region_set': region_set
        }

        # Load SA simulator configurations if they exist
        if config.has_option('Credentials', 'sa_simulator_region'):
            config_data['sa_simulator_region'] = config.get('Credentials', 'sa_simulator_region')
        
        if config.has_option('Credentials', 'sa_simulator_concurrent_simulations'):
            try:
                config_data['sa_simulator_concurrent_simulations'] = config.getint('Credentials', 'sa_simulator_concurrent_simulations')
            except ValueError:
                # If not an integer, try to get as string
                config_data['sa_simulator_concurrent_simulations'] = config.get('Credentials', 'sa_simulator_concurrent_simulations')

        # Load LLM configuration if section exists
        if config.has_section('LLM'):
            config_data.update({
                'llm_free_base_url': config.get('LLM', 'llm_free_base_url', fallback='https://api.deepseek.com'),
                'llm_free_api_key': config.get('LLM', 'llm_free_api_key', fallback=''),
                'llm_free_model': config.get('LLM', 'llm_free_model', fallback='deepseek-ai/DeepSeek-V3.2-Exp'),
                'llm_paid_base_url': config.get('LLM', 'llm_paid_base_url', fallback='https://api.openai.com/v1'),
                'llm_paid_api_key': config.get('LLM', 'llm_paid_api_key', fallback=''),
                'llm_paid_model': config.get('LLM', 'llm_paid_model', fallback='gpt-4'),
                'llm_base_url': config.get('LLM', 'llm_base_url', fallback='https://api.deepseek.com'),
                'llm_api_key': config.get('LLM', 'llm_api_key', fallback=''),
                'llm_model': config.get('LLM', 'llm_model', fallback='deepseek-chat')
            })

        # æ—¥å¿—è®°å½•å‚æ•°å€¼ï¼Œæ–¹ä¾¿è°ƒè¯•
        logger.info(f"Loaded config from {config_path}: username={config_data['username']}, "
                   f"max_concurrent={config_data['max_concurrent']} (type: {type(config_data['max_concurrent'])}), "
                   f"batch_number={config_data['batch_number_for_every_queue']} (type: {type(config_data['batch_number_for_every_queue'])}), "
                   f"init_date={config_data['init_date_str']}")

        return config_data

    except configparser.NoSectionError:
        logger.error(f"Section 'Credentials' not found in {config_path}")
        return None
    except configparser.NoOptionError as e:
        logger.error(f"Missing key in config: {e}")
        return None
    except ValueError as e:
        logger.error(f"Invalid value in config (must be integer for max_concurrent or batch_number): {e}")
        return None
    except Exception as e:
        logger.error(f"Error loading config: {e}")
        return None

def fix_newline_expression(expression):
    """
    ä¿®å¤è¡¨è¾¾å¼ä¸­çš„";n"é—®é¢˜ï¼ˆåº”è¯¥æ˜¯";\n"è¢«é”™è¯¯è½¬æ¢ï¼‰
    
    Args:
        expression (str): è¾“å…¥è¡¨è¾¾å¼å­—ç¬¦ä¸²
        
    Returns:
        str: ä¿®å¤åŽçš„è¡¨è¾¾å¼ï¼ˆå¦‚æžœå‘çŽ°é—®é¢˜ï¼‰ï¼Œå¦åˆ™è¿”å›žåŽŸè¡¨è¾¾å¼
    """
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨";n"é—®é¢˜
    if ";n" in expression:
        # æ›¿æ¢æ‰€æœ‰";n"ä¸º";\n"
        fixed_expression = expression.replace(";n", ";\n")
        return fixed_expression
    return expression

def negate_expression(expression):
    """
    ä¸ºè¡¨è¾¾å¼æ·»åŠ è´Ÿå·
    
    Args:
        expression (str): è¾“å…¥è¡¨è¾¾å¼å­—ç¬¦ä¸²
        
    Returns:
        str: å¤„ç†åŽçš„è¡¨è¾¾å¼
            å¦‚æžœæ˜¯å•è¡Œè¡¨è¾¾å¼ï¼Œç›´æŽ¥åœ¨å‰é¢åŠ è´Ÿå·
            å¦‚æžœæ˜¯å¤šè¡Œè¡¨è¾¾å¼ï¼Œåœ¨æœ€åŽä¸€è¡Œå‰åŠ è´Ÿå·
    """
    if "\n" not in expression:
        # å•è¡Œè¡¨è¾¾å¼
        return f"-({expression})"
    else:
        # å¤šè¡Œè¡¨è¾¾å¼
        lines = expression.split("\n")
        last_line = lines[-1].strip()
        # åœ¨æœ€åŽä¸€è¡Œå‰åŠ è´Ÿå·
        lines[-1] = f"-({last_line})"
        return "\n".join(lines)

def get_alphas_from_data(data_rows, min_sharpe, min_fitness, mode="track", region_filter=None, single_data_set_filter=None):
    """
    Process database data to generate alpha records in the format:
    [alpha_id, exp, sharpe, turnover, fitness, margin, dateCreated, decay]
    
    Args:
        data_rows: List of database rows containing alpha data
        min_sharpe: Minimum sharpe ratio threshold (e.g. 1.0)
        min_fitness: Minimum fitness threshold (e.g. 0.5)
        mode: Filter mode - 'submit' or 'track' (default: 'track')
        region_filter: Optional region filter (e.g. "USA"). If None, no region filtering.
        single_data_set_filter: Optional boolean to filter Single Data Set Alphas. If None, no filtering.
    
    Returns:
        List of filtered alpha records
    """
    output = []
    
    for row in data_rows:
        try:
            # Parse the necessary fields from the row
            alpha_id = row['id']
            
            # Parse the settings dictionary
            settings = row['settings']
            decay = settings.get('decay', 0)

            if settings.get('decay') == 0 and settings.get('neutralization') == 'NONE':
                continue
            
            # Parse the regular code (expression)
            regular = row['regular']
            exp = fix_newline_expression(regular.get('code', ''))
            operatorCount = regular.get('operatorCount', 0)
            
            # Parse the is dictionary
            is_data = row['is']
            sharpe = is_data.get('sharpe', 0)
            fitness = is_data.get('fitness', 0)
            turnover = is_data.get('turnover', 0)
            margin = is_data.get('margin', 0)
            longCount = is_data.get('longCount', 0)
            shortCount = is_data.get('shortCount', 0)
            checks = is_data.get('checks', [])
            
            dateCreated = row['dateCreated']
            has_failed_checks = any(check['result'] == 'FAIL' for check in checks)
            # Apply region filter if specified
            if region_filter is not None and settings.get('region') != region_filter:
                continue
                
            # Apply single data set filter if specified
            if single_data_set_filter is not None:
                classifications = row.get('classifications', '[]')
                is_single_data_set = any(
                    classification.get('id') == 'DATA_USAGE:SINGLE_DATA_SET' 
                    for classification in classifications
                )
                if single_data_set_filter != is_single_data_set:
                    continue
                elif operatorCount > 8:
                    continue
            
            # Apply other filters
            if (longCount + shortCount) > 100:
                if mode == "submit":
                    if (sharpe >= min_sharpe and fitness >= min_fitness) and not has_failed_checks:
                        if sharpe is not None and sharpe < 0:
                            exp = negate_expression(exp)
                        # Create the record
                        rec = [
                            alpha_id,
                            exp,
                            sharpe,
                            turnover,
                            fitness,
                            margin,
                            dateCreated,
                            decay
                        ]
                        
                        # Extract pyramids info from checks if available
                        pyramid_checks = [check for check in checks if check.get('name') == 'MATCHES_PYRAMID']
                        if pyramid_checks and 'pyramids' in pyramid_checks[0]:
                            pyramids = pyramid_checks[0]['pyramids']
                            rec.insert(7, pyramids)  # Insert after dateCreated
                        
                        # Add additional decay modifier based on turnover
                        if turnover > 0.7:
                            rec.append(decay * 4)
                        elif turnover > 0.6:
                            rec.append(decay * 3 + 3)
                        elif turnover > 0.5:
                            rec.append(decay * 3)
                        elif turnover > 0.4:
                            rec.append(decay * 2)
                        elif turnover > 0.35:
                            rec.append(decay + 4)
                        elif turnover > 0.3:
                            rec.append(decay + 2)
                        
                        output.append(rec)
                else:  # track mode
                    if (sharpe >= min_sharpe and fitness >= min_fitness) or (sharpe <= min_sharpe * -1.0 and fitness <= min_fitness * -1.0):
                        if sharpe is not None and sharpe < 0:
                            exp = negate_expression(exp)
                        # Create the record
                        rec = [
                            alpha_id,
                            exp,
                            sharpe,
                            turnover,
                            fitness,
                            margin,
                            dateCreated,
                            decay
                        ]
                        
                        # Extract pyramids info from checks if available
                        pyramid_checks = [check for check in checks if check.get('name') == 'MATCHES_PYRAMID']
                        if pyramid_checks and 'pyramids' in pyramid_checks[0]:
                            pyramids = pyramid_checks[0]['pyramids']
                            rec.insert(7, pyramids)  # Insert after dateCreated
                        
                        # Add additional decay modifier based on turnover
                        if turnover > 0.7:
                            rec.append(decay * 4)
                        elif turnover > 0.6:
                            rec.append(decay * 3 + 3)
                        elif turnover > 0.5:
                            rec.append(decay * 3)
                        elif turnover > 0.4:
                            rec.append(decay * 2)
                        elif turnover > 0.35:
                            rec.append(decay + 4)
                        elif turnover > 0.3:
                            rec.append(decay + 2)
                        
                        output.append(rec)
            
        except (ValueError, SyntaxError, KeyError) as e:
            # Skip rows with parsing errors or missing required fields
            continue
            
    return output

def safe_api_call(func, *args, max_retries=3, initial_delay=60, **kwargs):
    """
    Safely execute a function (usually an API call) with a retry mechanism.
    - func: The function to execute, e.g., ace_lib.get_check_submission.
    - *args, **kwargs: Arguments to pass to func.
    """
    delay = initial_delay
    for attempt in range(max_retries):
        try:
            # Directly call the function and return the result
            return func(*args, **kwargs)

        except (RemoteDisconnected, ConnectionError) as e:
            # Handle network errors
            print(f"Warning: API call failed with network error: {e}. Retrying in {delay}s... (Attempt {attempt + 1}/{max_retries})")

        except json.JSONDecodeError as e:
            # Handle JSON parsing errors
            print(f"Warning: Failed to decode JSON response: {e}. Retrying in {delay}s... (Attempt {attempt + 1}/{max_retries})")
        
        # If any of the above exceptions occur, wait and retry
        if attempt + 1 == max_retries:
            print(f"FATAL: API call failed after {max_retries} retries.")
            raise  # After exhausting retries, re-raise the last exception

        time.sleep(delay + random.uniform(0, 5))
        delay *= 2 # Exponentially increase delay

def extract_datafields(expression: str, brain_session) -> set:
    """
    (Final encapsulated version) Extracts a candidate set of data fields from an expression string.

    This function internally calls the API to get all operators and their keywords,
    then uses a "subtraction" logic: it finds all words in the expression,
    then subtracts all known "non-field" keywords, and the remaining are data fields.

    Args:
        expression: The alpha expression string.
        brain_session: The authenticated BRAIN platform session object.

    Returns:
        A set containing all candidate data fields.
    """
    # 1. --- å°è£…çš„é€»è¾‘ï¼šèŽ·å–å¹¶æž„å»ºç®—å­å…³é”®è¯é›†åˆ ---
    raw_operators_data = safe_api_call(ace.get_operators, brain_session)
    if raw_operators_data is None:
        print("Error: Could not fetch operators. Aborting field extraction.")
        return set()
    
    regular_operators_df = raw_operators_data[raw_operators_data['scope']=='REGULAR']
    regular_operators = regular_operators_df.to_dict('records')

    #print(regular_operators_df)
    
    operator_names = {op['name'] for op in regular_operators if op.get('name')}
    operator_params = set()
    for op in regular_operators:
        definition_str = op.get('definition')
        if definition_str:
            params = re.findall(r'(\w+)\s*=', definition_str)
            if params:
                operator_params.update(params)
    
    operator_keywords = operator_names.union(operator_params)
    
    # Manually add common BRAIN keywords/literals that are not fields
    non_field_keywords = operator_keywords.union({'true', 'false', 'and', 'or', 'nan', 'inf'})

    # print(f"Operator keywords ({len(operator_keywords)})")
    # --- å…³é”®è¯é›†åˆæž„å»ºå®Œæ¯• ---

    # 2. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾å‡ºæ‰€æœ‰å¯èƒ½æ˜¯æ ‡è¯†ç¬¦çš„â€œå•è¯â€
    # First, remove string literals (both single and double quoted) to avoid extracting their content as fields.
    expression_without_literals = re.sub(r'"[^"]*"|\'[^\']*\'', '', expression)
    all_identifiers = set(re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', expression_without_literals))
    
    # 3. ä»Žæ‰€æœ‰æ ‡è¯†ç¬¦ä¸­ï¼Œå‡åŽ»æ‰€æœ‰å·²çŸ¥çš„â€œéžå­—æ®µâ€å…³é”®è¯
    potential_fields = all_identifiers - non_field_keywords
    
    # 4. ä»Žå‰©ä¸‹çš„å€™é€‰ä¸­ï¼Œå†æŽ’é™¤çº¯æ•°å­—å¸¸é‡
    found_fields = set()
    for field in potential_fields:
        try:
            float(field) # å°è¯•è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        except ValueError:
            # å¦‚æžœè½¬æ¢å¤±è´¥ï¼Œè¯´æ˜Žå®ƒä¸æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªå­—æ®µ
            found_fields.add(field)
            
    return found_fields


def validate_expression_fields(expression: str, valid_data_fields: set, brain_session) -> Tuple[bool, Optional[str]]:
    """
    Validates that all parsed fields in a single expression exist in the given list of valid fields.
    
    Args:
        expression: The alpha expression string to validate.
        valid_data_fields: A set containing all valid data field names.
        brain_session: The authenticated BRAIN platform session object.

    Returns:
        Tuple[bool, Optional[str]]: A tuple where the first element is True if all fields are valid,
                                    False otherwise. The second element is an error message if validation fails,
                                    otherwise None.
    """
    try:
        # 1. Call internal function to extract candidate data fields used in the expression
        used_fields = extract_datafields(expression, brain_session)
    except Exception as e:
        # If an error occurs during parsing, return False with the error message
        error_message = f"Error parsing expression: {expression}, Error: {e}"
        print(f"âŒ {error_message}") # Keep print for debugging/logging
        return False, error_message

    # 2. Check if all extracted fields are present in the 'valid_data_fields' list
    unmatched_fields = used_fields - valid_data_fields
    
    # 3. Return result based on the check
    if not unmatched_fields:
        # If the difference set is empty, all fields matched successfully, expression is valid
        return True, None
    else:
        # Otherwise, return False with the list of unmatched fields
        error_message = f"Expression filtered: Undefined or invalid fields found: {unmatched_fields}"
        print(f"âŒ {error_message}") # Keep print for debugging/logging
        return False, error_message


def filter_alpha_by_datafields(alpha_expression: str, settings_dict: dict, brain_session) -> bool:
    """
    Filters an alpha expression based on its datafields to check if it only uses pv1 fields.

    Args:
        alpha_expression: The alpha expression string.
        settings_dict: A dictionary containing settings like region, universe, delay.
        brain_session: The authenticated BRAIN platform session object.

    Returns:
        bool: True if the alpha should be simulated (contains non-pv1 fields), 
              False if it should be filtered out (contains only pv1 fields).
    """
    # 1. Extract datafields from the expression
    try:
        used_fields = extract_datafields(alpha_expression, brain_session)
        if not used_fields:
            # If no fields are found, no need to filter, let it be validated later.
            return True
    except Exception as e:
        print(f"Error parsing expression during pv1 filter, skipping filter: {alpha_expression}, Error: {e}")
        # If parsing fails, don't filter it out, let it fail in the main validation.
        return True

    # 2. Get grouping and price fields from the pv1 dataset
    region = settings_dict.get("region", "GLB")
    universe = settings_dict.get("universe", "TOP3000")
    delay = settings_dict.get("delay", 1)

    try:
        grouping_fields_df = get_datafields_with_cache(brain_session, region=region, universe=universe, delay=delay, data_type='GROUP', dataset_id='pv1')
        price_fields_df = get_datafields_with_cache(brain_session, region=region, universe=universe, delay=delay, data_type='MATRIX', dataset_id='pv1')
    except Exception as e:
        print(f"Error getting grouping/price fields for pv1 filter, skipping filter: {e}")
        # If getting base fields fails, don't filter as a precaution.
        return True
        
    grouping_fields_ids = set(grouping_fields_df['id']) if grouping_fields_df is not None and not grouping_fields_df.empty else set()
    price_fields_ids = set(price_fields_df['id']) if price_fields_df is not None and not price_fields_df.empty else set()

    pv1_fields = grouping_fields_ids.union(price_fields_ids)

    # 3. Check if all used fields are a subset of the pv1 fields
    if pv1_fields and used_fields.issubset(pv1_fields):
        print(f"Expression filtered: '{alpha_expression}' contains only fields from the 'pv1' dataset.")
        return False
    else:
        # If there are fields not in pv1, or if pv1 fields could not be fetched, let it pass
        return True



def complete_expression(expression: str, all_fields_df: pd.DataFrame, brain_session) -> list[str]:
    """
    æ ¹æ®æ•°æ®å­—æ®µçš„ç±»åž‹ï¼ˆVECTOR/MATRIXï¼‰è‡ªåŠ¨è¡¥å…¨æˆ–ä¿®æ­£å•ä¸ªè¡¨è¾¾å¼ã€‚
    è§„åˆ™:
    1. å¦‚æžœVECTORç±»åž‹çš„æ•°æ®å­—æ®µæ²¡æœ‰è¢«vec_**()å½¢å¼çš„ç®—å­åŒ…è£¹ï¼Œåˆ™ä¸ºå…¶æ·»åŠ vec_avg()ç®—å­ã€‚
    2. å¦‚æžœMATRIXç±»åž‹çš„æ•°æ®å­—æ®µè¢«vec_**()å½¢å¼çš„ç®—å­åŒ…è£¹ï¼Œåˆ™åŽ»æŽ‰è¯¥vec_å¼€å¤´çš„ç®—å­ã€‚
    
    Args:
        expression: è¦ä¿®æ­£çš„alphaè¡¨è¾¾å¼å­—ç¬¦ä¸²ã€‚
        all_fields_df: åŒ…å«æ‰€æœ‰å­—æ®µåŠå…¶ç±»åž‹çš„DataFrameã€‚
        brain_session: å·²è®¤è¯çš„BRAINå¹³å°ä¼šè¯å¯¹è±¡ã€‚

    Returns:
        list[str]: ä¸€ä¸ªåŒ…å«ä¿®æ­£åŽè¡¨è¾¾å¼çš„åˆ—è¡¨ï¼ˆä¸ºæœªæ¥æ‰©å±•æ€§è®¾è®¡ï¼‰ã€‚
    """
    # 1. ä»Žä¼ å…¥çš„DataFrameåˆ›å»ºå­—æ®µç±»åž‹æ˜ å°„
    field_type_map = pd.Series(all_fields_df.type.values, index=all_fields_df.id).to_dict()

    # 2. èŽ·å–è¡¨è¾¾å¼ä¸­ä½¿ç”¨çš„æ•°æ®å­—æ®µ
    try:
        used_fields = extract_datafields(expression, brain_session)
    except Exception as e:
        print(f"âŒ åœ¨è¡¥å…¨å‰è§£æžè¡¨è¾¾å¼æ—¶å‡ºé”™: {expression}, é”™è¯¯: {e}")
        return [expression] # å‡ºé”™åˆ™è¿”å›žåŽŸè¡¨è¾¾å¼

    # 3. åº”ç”¨ä¿®æ­£è§„åˆ™
    new_expr = expression
    for field in used_fields:
        field_type = field_type_map.get(field)
        
        if field_type == 'VECTOR':
            # è§„åˆ™ä¸€ï¼šä¸ºè£¸éœ²çš„VECTORå­—æ®µæ·»åŠ vec_avg()ã€‚
            # æ­¥éª¤1: é¦–å…ˆå°†æ‰€æœ‰å‡ºçŽ°çš„è¯¥å­—æ®µç”¨vec_avg()åŒ…è£¹ã€‚
            # ä¾‹å¦‚: vec_sum(v1) + v1  ->  vec_sum(vec_avg(v1)) + vec_avg(v1)
            temp_expr = re.sub(r'\b' + re.escape(field) + r'\b', f'vec_avg({field})', new_expr)
            
            # æ­¥éª¤2: ä¿®æ­£æ­¥éª¤1ä¸­å¯èƒ½é€ æˆçš„åŒé‡åŒ…è£¹é—®é¢˜ã€‚
            # ä¾‹å¦‚: vec_sum(vec_avg(v1)) -> vec_sum(v1)
            # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾ vec_xxx(vec_avg(field)) çš„æ¨¡å¼
            double_wrap_pattern = r'vec_(\w+)\s*\(\s*vec_avg\s*\(\s*(' + re.escape(field) + r')\s*\)\s*\)'
            # å¹¶å°†å…¶æ›¿æ¢ä¸º vec_xxx(field)
            new_expr = re.sub(double_wrap_pattern, r'vec_\1(\2)', temp_expr)

        elif field_type == 'MATRIX':
            # è§„åˆ™äºŒï¼šä¸ºè¢«vec_**()åŒ…è£¹çš„MATRIXå­—æ®µç§»é™¤åŒ…è£¹ã€‚
            # ä½¿ç”¨whileå¾ªçŽ¯æ¥å¤„ç†å¯èƒ½å­˜åœ¨çš„å¤šå±‚åŒ…è£¹ï¼Œä¾‹å¦‚ vec_sum(vec_avg(m1))
            pattern_to_remove = r'vec_\w+\s*\(\s*(' + re.escape(field) + r')\s*\)'
            while re.search(pattern_to_remove, new_expr):
                new_expr = re.sub(pattern_to_remove, r'\1', new_expr)
    
    if new_expr != expression:
        print(f"ðŸ”§ è¡¨è¾¾å¼å·²ä¿®æ­£: '{expression}' -> '{new_expr}'")
    
    new_expr = update_expression_parameters(new_expr)
    # ä»¥åˆ—è¡¨å½¢å¼è¿”å›žï¼Œä¾¿äºŽæœªæ¥æ‰©å±•
    return [new_expr]


def update_expression_parameters(expression: str) -> str:
    """
    Update expression parameters to explicitly name them.
    """
    # ts_regression with 5 parameters: e.g. ts_regression(y, x, d, lag, rettype) -> ts_regression(y, x, d, lag=1, rettype=2)
    expression = re.sub(r'ts_regression\(([^,]+,\s*[^,]+,\s*[^,]+),\s*(\d+),\s*(\d+)\)$', r'ts_regression(\1, lag=\2, rettype=\3)', expression)
    # ts_regression with 4 parameters: e.g. ts_regression(y, x, d, rettype) -> ts_regression(y, x, d, rettype=1)
    expression = re.sub(r'ts_regression\(([^,]+,\s*[^,]+,\s*[^,]+),\s*(\d+)\)$', r'ts_regression(\1, rettype=\2)', expression)
    # winsorize
    expression = re.sub(r'winsorize\(([^,]+,)\s*(\d+(\.\d+)?)\)$', r'winsorize(\1 std=\2)', expression)
    # group_backfill
    expression = re.sub(r'group_backfill\(([^,]+,)\s*(\d+(\.\d+)?)\)$', r'group_backfill(\1 std=\2)', expression)
    return expression

# Use ace to query datasets based on fields in alpha, return dataset DataFrame
def get_datasets_for_alpha(alpha_id: str, brain_session) -> pd.DataFrame:
    """
    Queries datasets based on the data fields used in a given alpha expression.
    This function is wrapped with safe_api_call for robustness.
    """
    def _get_json(session, url):
        """Helper to get and decode JSON from a URL."""
        response = session.get(url)
        response.raise_for_status()
        return response.json()

    # Safely get alpha details
    alpha_details = safe_api_call(_get_json, brain_session, f"https://api.worldquantbrain.com/alphas/{alpha_id}")
    if not alpha_details:
        print(f"Error: Could not fetch details for alpha {alpha_id}. Aborting.")
        return pd.DataFrame(), None
        
    alpha_expression = alpha_details['regular']['code']
    datafields = extract_datafields(alpha_expression, brain_session)
    print(f"Data fields for alpha {alpha_id}: {datafields}")
    settings_dict = extract_alpha_settings(alpha_details)
    
    # Use search parameter to get data fields, then keep only exact matches
    data_fields_list = []
    for data_field in datafields:
        search_results = ace.get_datafields(brain_session, region=settings_dict['region'], universe=settings_dict['universe'], delay=settings_dict['delay'], data_type='ALL', search=data_field)
        # Keep only exact matches in the search results
        exact_matches = search_results[search_results['id'] == data_field]
        if not exact_matches.empty:
            data_fields_list.append(exact_matches)
    
    data_fields = pd.DataFrame()
    if data_fields_list:
        data_fields = pd.concat(data_fields_list, ignore_index=True)
    
    print(f"Exact matched data fields details:")
    print(f"Number of matches: {len(data_fields)}")
    if not data_fields.empty:
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        print(data_fields.to_string())
    
    print("\n" + "="*50)
    print("Next step: Query datasets based on dataset_id")
    print("="*50)
    
    if data_fields.empty:
        return pd.DataFrame(), None

    dataset_ids = list(set(data_fields['dataset_id'].tolist()))
    dataset_ids = [did for did in dataset_ids if did != 'pv1']
    print(f"Dataset IDs (after deduplication and filtering 'pv1'): {dataset_ids}")

    if not dataset_ids:
        return pd.DataFrame(), None

    datasets_list = []
    for dataset_id in dataset_ids:
        dataset_info = get_datafields_with_cache(
            brain_session, 
            region=settings_dict['region'],
            universe=settings_dict['universe'],
            delay=settings_dict['delay'],
            data_type='ALL',
            dataset_id=dataset_id
        )
        
        if dataset_info is not None and not dataset_info.empty:
            datasets_list.append(dataset_info)
    
    all_datasets = pd.concat(datasets_list, ignore_index=True) if datasets_list else pd.DataFrame()
    
    print(f"\nAll datasets information:")
    print(f"Number of datasets: {len(datasets_list)}")

    return all_datasets, dataset_ids[0] if dataset_ids else None

def extract_alpha_settings(alpha_details):
    """
    Extracts setting information from the original signal.

    Args:
        alpha_details: The original signal alpha_details.

    Returns:
        A dictionary where the key is the signal ID and the value is another dictionary
        containing region, universe, neutralization, maxTrade, visualization, and delay.
    """
    try:
        # Parse the setting string into a dictionary
        settings = alpha_details['settings']
        # Extract the required fields
        settings_dict = {
            'region': settings.get('region'),
            'universe': settings.get('universe'),
            'neutralization': settings.get('neutralization'),
            'maxTrade': settings.get('maxTrade'),
            'visualization': settings.get('visualization'),
            'delay': settings.get('delay', 1),  # Default value is 1
            'decay': settings.get('decay', 0)  # Default value is 0
        }
    except Exception as e:
        print(f"Error parsing settings for alpha id {alpha_details['id']}: {e}")
    return settings_dict

def get_dataset_ids(datasets_df: pd.DataFrame) -> list:
    """
    Extracts all field values from the 'id' column of a dataset DataFrame.

    Args:
        datasets_df: The dataset DataFrame returned from the get_datasets_for_alpha method.

    Returns:
        list: A list containing all values from the 'id' field.
    """
    if datasets_df is None or datasets_df.empty:
        return []
    
    # Check if the DataFrame contains an 'id' column
    if 'id' not in datasets_df.columns:
        return []
    
    # Extract all values from the 'id' column and return as a list
    return datasets_df['id'].tolist()

def sort_alphas_by_fitness(alpha_ids: List[str], brain_session) -> List[str]:
    """
    Sorts a list of alpha IDs by their 'fitness' value in descending order.

    Args:
        alpha_ids: A list of alpha ID strings.
        brain_session: The authenticated BRAIN platform session object.

    Returns:
        A list of alpha IDs sorted by fitness score from highest to lowest.
    """
    def _get_json_from_url(session, url):
        """Helper to get and decode JSON from a URL."""
        response = session.get(url)
        response.raise_for_status()
        return response.json()

    alpha_fitness_list = []
    for alpha_id in alpha_ids:
        try:
            alpha_details = safe_api_call(_get_json_from_url, brain_session, f"https://api.worldquantbrain.com/alphas/{alpha_id}")
            if alpha_details:
                is_stats = alpha_details.get('is', {})
                fitness = is_stats.get('fitness')
                if fitness is not None:
                    alpha_fitness_list.append((alpha_id, fitness))
                else:
                    print(f"Warning: 'fitness' not found for alpha {alpha_id}.")
            else:
                print(f"Warning: Could not retrieve details for alpha {alpha_id}.")
        except Exception as e:
            print(f"Error fetching details for alpha {alpha_id}: {e}")

    # Sort the list of tuples by fitness in descending order
    alpha_fitness_list.sort(key=lambda x: x[1], reverse=True)

    # Extract just the alpha IDs from the sorted list
    sorted_alpha_ids = [alpha_id for alpha_id, fitness in alpha_fitness_list]

    return sorted_alpha_ids

def filter_and_fix_expressions(expression_list: List[str], region: str, universe: str, delay: int, dataset_id: str, brain_session) -> List[str]:
    """
    Method 1: Filter expressions based on valid fields and fix vector type fields.
    
    Args:
        expression_list (List[str]): List of alpha expressions.
        region (str): Region code.
        universe (str): Universe code.
        delay (int): Delay value.
        dataset_id (str): Dataset ID.
        brain_session: Authenticated BRAIN session.
        
    Returns:
        List[str]: List of filtered and fixed expressions.
    """
    # 1. Get valid datafields
    print(f"Fetching datafields for dataset {dataset_id}...")
    df = get_datafields_with_cache(brain_session, region=region, universe=universe, delay=delay, data_type='ALL', dataset_id=dataset_id)
    if df is None or df.empty:
        print(f"Error: No datafields found for dataset {dataset_id}")
        return []
        
    valid_field_ids = set(df['id'])
    
    fixed_expressions_list = []
    
    print(f"Iterating through {len(expression_list)} expressions to validate and fix...")
    for i, expression in enumerate(expression_list):
        if i % 10 == 0:
            print(f"Processing expression {i+1}/{len(expression_list)}...")
        # 2. Validate fields
        is_valid, _ = validate_expression_fields(expression, valid_field_ids, brain_session)
        
        if is_valid:
            # 3. Fix expression (complete_expression returns a list)
            fixed_exprs = complete_expression(expression, df, brain_session)
            fixed_expressions_list.extend(fixed_exprs)
            
    return fixed_expressions_list

def save_simulatable_alphas(expression_list: List[str], region: str, universe: str, delay: int, dataset_id: str, decay: int, neutralization: str, truncation: float, max_trade):
    """
    Method 2: Assemble expressions and settings into simulatable_alphas.json format and save.
    
    Args:
        expression_list (List[str]): List of alpha expressions.
        region (str): Region code.
        universe (str): Universe code.
        delay (int): Delay value.
        dataset_id (str): Dataset ID (passed for context/consistency, currently unused in output).
        decay (int): Decay value.
        neutralization (str): Neutralization method.
        truncation (float): Truncation value.
        max_trade (float/str): Max trade value.
    """
    print(f"Entering save_simulatable_alphas with {len(expression_list)} expressions")
    # Define output path
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, "output", f"simulatable_alphas_{region}_{universe}_{delay}_{dataset_id}.json")
    
    alphas_to_save = []
    
    for expr in expression_list:
        alpha_entry = {
            "type": "REGULAR",
            "settings": {
                "instrumentType": "EQUITY",
                "region": region,
                "universe": universe,
                "delay": delay,
                "decay": decay,
                "neutralization": neutralization,
                "truncation": truncation,
                "pasteurization": "ON",
                "unitHandling": "VERIFY",
                "nanHandling": "OFF",
                "language": "FASTEXPR",
                "visualization": False,
                "maxTrade": max_trade
            },
            "regular": expr
        }
        alphas_to_save.append(alpha_entry)
        
    # Assemble into the final structure: Region -> Universe -> List of Alphas
    final_data = {
        region: {
            universe: alphas_to_save
        }
    }
    
    # Ensure directory exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Write to file
    try:
        with open(output_path, 'w') as f:
            json.dump(final_data, f, indent=4)
        print(f"Successfully saved {len(alphas_to_save)} alphas to {output_path}")
    except Exception as e:
        print(f"Error saving simulatable alphas to {output_path}: {e}")

def main():
    # 1. Load config for credentials
    brain_session = ace.start_session()

    # 3. Define variables
    expression_list = [
        "ts_corr(fnd28_value_01551a / fnd28_value_01001a, fnd28_value_04860a / fnd28_value_01001a, 63)",
        "(fnd28_value_04551a / fnd28_value_01551a) * (fnd28_value_04860a / fnd28_value_01001a)",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 22) / ts_delay(fnd28_value_01250a / fnd28_value_01001a, 22)",
        "fnd28_value_02501a / fnd28_value_02999a",
        "ts_delta(-1 * (fnd28_value_03351a / fnd28_value_02999a) * (fnd28_value_04860a / fnd28_value_02999a), 63)",
        "ts_std_dev(fnd28_value_04860a / fnd28_value_01001a, 252)",
        "fnd28_value_01250a / fnd28_value_01001a",
        "ts_delta(fnd28_value_01001a, 252) / ts_delay(fnd28_value_01001a, 252)",
        "fnd28_value_03495a / fnd28_value_03501a",
        "(fnd28_value_01551a / fnd28_value_03501a) / (fnd28_value_01551a / fnd28_value_02999a)",
        "ts_rank(fnd28_value_01250a / fnd28_value_01001a, 63)",
        "fnd28_value_02101a / fnd28_value_02999a",
        "fnd28_value_01551a / fnd28_value_02999a",
        "ts_std_dev(fnd28_value_04860a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_04860a / fnd28_value_01001a, 252))",
        "ts_rank(-1 * (fnd28_value_03351a / fnd28_value_02999a) * (fnd28_value_04860a / fnd28_value_02999a), 252) - 0.5",
        "ts_rank((fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252), 252) - 0.5",
        "fnd28_value_01250a / fnd28_value_02999a",
        "ts_rank(fnd28_value_01250a / fnd28_value_01001a, 252)",
        "ts_std_dev(fnd28_value_01551a / fnd28_value_03501a, 252) / abs(ts_mean(fnd28_value_01551a / fnd28_value_03501a, 252))",
        "fnd28_value_01551a / fnd28_value_01001a",
        "fnd28_value_04551a / fnd28_value_03501a",
        "fnd28_value_01250a / fnd28_value_01401a",
        "ts_std_dev(fnd28_value_04601a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_04601a / fnd28_value_01001a, 252))",
        "ts_delta(fnd28_value_01250a / fnd28_value_02999a, 63) * ts_corr(fnd28_value_04601a / fnd28_value_02999a, ts_delta(fnd28_value_01250a, 63), 126)",
        "ts_rank(fnd28_value_04860a / fnd28_value_01001a, 63)",
        "(fnd28_value_01001a - fnd28_value_01051a) / fnd28_value_01001a",
        "(-1 * (fnd28_value_03351a / fnd28_value_02999a) * (fnd28_value_04860a / fnd28_value_02999a)) / ts_mean(-1 * (fnd28_value_03351a / fnd28_value_02999a) * (fnd28_value_04860a / fnd28_value_02999a), 252) - 1",
        "ts_delta(fnd28_value_01001a / fnd28_value_02999a, 63)",
        "ts_delta(ts_mean(fnd28_value_01551a / fnd28_value_01001a, 252), 252)",
        "(fnd28_value_01250a / fnd28_value_01001a - ts_min(fnd28_value_01250a / fnd28_value_01001a, 252)) / (ts_max(fnd28_value_01250a / fnd28_value_01001a, 252) - ts_min(fnd28_value_01250a / fnd28_value_01001a, 252))",
        "ts_delta(ts_delta(fnd28_value_01250a / fnd28_value_01001a, 22), 22)",
        "fnd28_value_04860a / fnd28_value_01551a",
        "fnd28_value_01250a * 0.7 - fnd28_value_02999a * 0.1",
        "(fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252) * ts_rank(fnd28_value_01201a / fnd28_value_01001a, 252)",
        "fnd28_value_03351a / fnd28_value_02999a",
        "ts_std_dev(fnd28_value_01250a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_01250a / fnd28_value_01001a, 252))",
        "-1 * (fnd28_value_03351a / fnd28_value_02999a) * ts_rank(fnd28_value_04860a / fnd28_value_02999a, 126) * (fnd28_value_02001a / fnd28_value_03101a)",
        "ts_delta(fnd28_value_04860a, 252) / ts_delay(fnd28_value_04860a, 252)",
        "(fnd28_value_04860a - fnd28_value_01551a) / fnd28_value_01001a",
        "fnd28_value_04860a - fnd28_value_04601a",
        "fnd28_value_04860a / fnd28_value_03351a",
        "ts_std_dev(fnd28_value_02001a / fnd28_value_03101a, 252) / abs(ts_mean(fnd28_value_02001a / fnd28_value_03101a, 252))",
        "fnd28_value_01001a / fnd28_value_01084a",
        "(ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252) * ts_corr(fnd28_value_04860a, fnd28_value_01551a, 63)",
        "(fnd28_value_04860a / fnd28_value_01551a - ts_mean(fnd28_value_04860a / fnd28_value_01551a, 252)) / ts_std_dev(fnd28_value_04860a / fnd28_value_01551a, 252)",
        "(fnd28_value_04860a - fnd28_value_01551a) / ts_std_dev(fnd28_value_04860a, 252)",
        "fnd28_value_01250a / fnd28_value_01001a - ts_min(fnd28_value_01250a / fnd28_value_01001a, 252)",
        "(fnd28_value_01201a / fnd28_value_01001a - ts_mean(fnd28_value_01201a / fnd28_value_01001a, 252)) / ts_std_dev(fnd28_value_01201a / fnd28_value_01001a, 252)",
        "fnd28_value_02001a / fnd28_value_02999a",
        "((fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252)) * ((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252))",
        "fnd28_value_01201a / fnd28_value_01001a",
        "ts_sum(fnd28_value_01201a, 1008) / fnd28_value_02999a",
        "fnd28_value_02001a / fnd28_value_03101a",
        "(fnd28_value_02001a + fnd28_value_02051a) / fnd28_value_02999a - (fnd28_value_02501a + fnd28_value_02502a) / fnd28_value_02999a",
        "ts_rank(fnd28_value_01250a / fnd28_value_01001a, 252) - 0.5",
        "ts_delta(ts_mean(fnd28_value_01250a / fnd28_value_01001a, 22), 22)",
        "ts_corr(fnd28_value_01250a / fnd28_value_01001a, fnd28_value_04860a / fnd28_value_01001a, 63)",
        "ts_delta((fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252), 63)",
        "fnd28_value_01551a / fnd28_value_03501a",
        "fnd28_value_04860a / fnd28_value_04551a",
        "ts_std_dev(ts_delta(fnd28_value_01551a, 252) / ts_delay(fnd28_value_01551a, 252), 252) / abs(ts_mean(ts_delta(fnd28_value_01551a, 252) / ts_delay(fnd28_value_01551a, 252), 252))",
        "ts_std_dev((fnd28_value_01001a - fnd28_value_01051a) / fnd28_value_01001a, 252) / abs(ts_mean((fnd28_value_01001a - fnd28_value_01051a) / fnd28_value_01001a, 252))",
        "ts_delta(fnd28_value_01250a / fnd28_value_02999a, 63) * (fnd28_value_04860a / fnd28_value_04601a)",
        "ts_max(fnd28_value_01250a / fnd28_value_01001a, 252) - fnd28_value_01250a / fnd28_value_01001a",
        "(fnd28_value_02001a + fnd28_value_02051a) / fnd28_value_03101a",
        "ts_std_dev(fnd28_value_03351a / fnd28_value_02999a, 252)",
        "ts_std_dev(fnd28_value_01201a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_01201a / fnd28_value_01001a, 252))",
        "((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252)) * (fnd28_value_04860a / fnd28_value_01001a)",
        "ts_rank(fnd28_value_01250a / fnd28_value_02999a, 63)",
        "fnd28_value_01250a / fnd28_value_01001a - ts_mean(fnd28_value_01250a / fnd28_value_01001a, 252)",
        "(fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252)",
        "(fnd28_value_01250a / fnd28_value_01001a) * ((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252)) * (-1 * (fnd28_value_03351a / fnd28_value_02999a) * (fnd28_value_04860a / fnd28_value_02999a))",
        "fnd28_value_01001a / fnd28_value_02999a",
        "ts_rank((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252), 252) - 0.5",
        "(ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252)",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 63)",
        "fnd28_value_04601a / fnd28_value_01001a",
        "(fnd28_value_04860a / fnd28_value_01001a - ts_mean(fnd28_value_04860a / fnd28_value_01001a, 252)) / ts_std_dev(fnd28_value_04860a / fnd28_value_01001a, 252)",
        "ts_delta(fnd28_value_01551a, 252) / ts_delay(fnd28_value_01551a, 252)",
        "ts_std_dev(fnd28_value_01551a / fnd28_value_02999a, 252) / abs(ts_mean(fnd28_value_01551a / fnd28_value_02999a, 252))",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 5) - ts_delta(fnd28_value_01250a / fnd28_value_01001a, 22)",
        "fnd28_value_01250a / fnd28_value_01084a",
        "(fnd28_value_01250a / fnd28_value_01001a - ts_mean(fnd28_value_01250a / fnd28_value_01001a, 252)) / ts_std_dev(fnd28_value_01250a / fnd28_value_01001a, 252)",
        "fnd28_value_04860a / fnd28_value_01001a",
        "ts_std_dev(fnd28_value_03501a / fnd28_value_02999a, 252)",
        "ts_delta(ts_mean(fnd28_value_01250a / fnd28_value_01001a, 22), 5) - ts_delta(ts_mean(fnd28_value_01250a / fnd28_value_01001a, 63), 22)",
        "fnd28_value_02502a / fnd28_value_02999a",
        "ts_std_dev(ts_delta(fnd28_value_04860a, 252) / ts_delay(fnd28_value_04860a, 252), 252) / abs(ts_mean(ts_delta(fnd28_value_04860a, 252) / ts_delay(fnd28_value_04860a, 252), 252))",
        "ts_std_dev(fnd28_value_01151a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_01151a / fnd28_value_01001a, 252))",
        "ts_delta(fnd28_value_01551a / fnd28_value_01001a, 63) / ts_delay(fnd28_value_01551a / fnd28_value_01001a, 63)",
        "ts_delta((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252), 63)",
        "fnd28_value_01250a / ts_mean(fnd28_value_01250a, 252)",
        "ts_delta(ts_delta(fnd28_value_01250a / fnd28_value_01001a, 63), 63)",
        "ts_delta(fnd28_value_01250a, 252) / ts_delay(fnd28_value_01250a, 252)",
        "(fnd28_value_01084a / fnd28_value_01001a - ts_mean(fnd28_value_01084a / fnd28_value_01001a, 252)) / ts_std_dev(fnd28_value_01084a / fnd28_value_01001a, 252)",
        "((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252)) / ts_mean((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252), 252) - 1",
        "ts_std_dev(fnd28_value_03351a / fnd28_value_03501a, 252) / abs(ts_mean(fnd28_value_03351a / fnd28_value_03501a, 252))",
        "ts_corr(fnd28_value_01250a / fnd28_value_01001a, fnd28_value_01551a / fnd28_value_01001a, 63)",
        "ts_rank(fnd28_value_01551a / fnd28_value_03501a, 63)",
        "ts_std_dev(fnd28_value_01551a / fnd28_value_01001a, 252)",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 5) / ts_delay(fnd28_value_01250a / fnd28_value_01001a, 5)",
        "fnd28_value_01001a / fnd28_value_02101a",
        "fnd28_value_01250a / fnd28_value_01001a - ts_mean(fnd28_value_01250a / fnd28_value_01001a, 60)",
        "ts_std_dev(fnd28_value_01001a / fnd28_value_02999a, 252) / abs(ts_mean(fnd28_value_01001a / fnd28_value_02999a, 252))",
        "ts_delta(ts_delta(fnd28_value_01551a / fnd28_value_01001a, 63), 63)",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 22)",
        "fnd28_value_03351a / fnd28_value_03501a",
        "ts_std_dev(fnd28_value_01250a / fnd28_value_01001a, 252)",
        "ts_delta(ts_mean(fnd28_value_01250a / fnd28_value_01001a, 63), 63)",
        "ts_std_dev(fnd28_value_01250a / fnd28_value_01001a, 60)",
        "fnd28_value_04551a / fnd28_value_01551a",
        "fnd28_value_04860a / fnd28_value_01250a",
        "fnd28_value_01551a / fnd28_value_01084a",
        "ts_rank(fnd28_value_01551a / fnd28_value_01001a, 63)",
        "(fnd28_value_01551a / fnd28_value_03501a) / ts_std_dev(fnd28_value_01551a / fnd28_value_03501a, 252)",
        "(fnd28_value_01551a / fnd28_value_01001a - ts_mean(fnd28_value_01551a / fnd28_value_01001a, 252)) / ts_std_dev(fnd28_value_01551a / fnd28_value_01001a, 252)",
        "ts_std_dev(fnd28_value_04551a / fnd28_value_01551a, 252) / abs(ts_mean(fnd28_value_04551a / fnd28_value_01551a, 252))",
        "ts_delta(ts_delta(fnd28_value_01250a / fnd28_value_01001a, 5), 5)",
        "ts_delta(fnd28_value_01250a / fnd28_value_01001a, 22) - ts_delta(fnd28_value_01250a / fnd28_value_01001a, 63)",
        "ts_std_dev(fnd28_value_01551a / fnd28_value_01001a, 252) / abs(ts_mean(fnd28_value_01551a / fnd28_value_01001a, 252))",
        "fnd28_value_01001a / fnd28_value_02051a",
        "((fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252)) / ts_mean((fnd28_value_01250a - ts_mean(fnd28_value_01250a, 252, 1)) / ts_delay(fnd28_value_01201a, 252), 252) - 1",
        "(fnd28_value_01250a / fnd28_value_01001a) / ts_mean(fnd28_value_01250a / fnd28_value_01001a, 252) - 1",
        "ts_std_dev(ts_delta(fnd28_value_01001a, 252) / ts_delay(fnd28_value_01001a, 252), 252) / abs(ts_mean(ts_delta(fnd28_value_01001a, 252) / ts_delay(fnd28_value_01001a, 252), 252))",
        "ts_delta(ts_delta((ts_delta(fnd28_value_01001a, 252) - ts_delta(fnd28_value_02502a, 252)) / ts_delay(fnd28_value_01001a, 252), 63), 63)"
    ]
    region = "HKG"
    universe = "TOP800"
    delay = 1
    dataset_id = "fundamental28" # Example dataset ID
    decay = 20
    neutralization = "REVERSION_AND_MOMENTUM"
    truncation = 0.01
    max_trade = "OFF"

    print(f"Processing {len(expression_list)} expressions...")

    # 4. Filter and fix expressions
    print("Calling filter_and_fix_expressions...")
    fixed_expressions = filter_and_fix_expressions(
        expression_list=expression_list,
        region=region,
        universe=universe,
        delay=delay,
        dataset_id=dataset_id,
        brain_session=brain_session
    )

    print(f"Filtered and fixed {len(fixed_expressions)} expressions.")

    # 5. Save simulatable alphas
    print("Calling save_simulatable_alphas...")
    save_simulatable_alphas(
        expression_list=fixed_expressions,
        region=region,
        universe=universe,
        delay=delay,
        dataset_id=dataset_id,
        decay=decay,
        neutralization=neutralization,
        truncation=truncation,
        max_trade=max_trade
    )
    print("Finished save_simulatable_alphas.")

if __name__ == "__main__":
    main()
